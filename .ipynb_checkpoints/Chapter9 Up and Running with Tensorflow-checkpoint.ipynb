{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"Chapter9\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and running a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter (or in a Python shell), it is common to run the same commands more than once while we are experimenting. As a result, we may end up with default graph containing many duplicate nodes. One solution is to restart the Jupyter kernel (or the Python shell), but a more convenient solution is to just reset the default graph by running tf.rest_default_graph().\n",
    "\n",
    "The following code does not actually perform any computation. It just creates a computation graph. In fact, even the variables are note initialized yet. To evaluate this graph, we need to open a Tensorflow session and use it to initialize the variables and evaluate f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to repeat sess.run() all the time is a bit cumbersome, but there is a better way:\n",
    "\n",
    "Inside the with block, the session is set as the default session. Calling x.initializer.run() is equivalent to calling tf.get_default_session().run(x.initializer), and similary f.eval() is equivalent to calling tf.get_default_session().run(f). This makes the code easier to read. Moreover, the session is automatically closed at the end of the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually running the initializer for every single variable, we can use the global_variables_initializer() function. Note that it does not actually perform the initialization immediately, but rather creates a node in the graph that will initialize all variables when it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # acutually initialize all the variables\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside Jupyter or within a Python shell we may prefer create an InteractiveSession. The only difference from a regular Session is that when an InteactiveSession is created it automatically sets itself as the default session, so we don't need a with block (but we do need to close the session manually when we are done with it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow program is typically split into two parts: the first part builds a computation graph (this is called the construction phase), and the second part runs it (this is the execution phase). The construction phase typically builds a computation graph representing the ML model and the computations required to train it. The execution phase generally runs a loop that evaluates a training step repeatedly (e.g. one step per mini-batch), gradually improving the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any node we create is automatically added to the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases this is fine, but sometimes we may want to manage multiple independent graphs. We can do this by creating a new Graph and temporarily making it the default graph inside a with block, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.74651413e+01],\n",
       "       [  4.35734153e-01],\n",
       "       [  9.33829229e-03],\n",
       "       [ -1.06622010e-01],\n",
       "       [  6.44106984e-01],\n",
       "       [ -4.25131839e-06],\n",
       "       [ -3.77322501e-03],\n",
       "       [ -4.26648885e-01],\n",
       "       [ -4.40514028e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare with pure NumPy\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare with Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   6.60969987e-17   5.50808322e-18   6.60969987e-17\n",
      "  -1.06030602e-16  -1.10161664e-17   3.44255201e-18  -1.07958431e-15\n",
      "  -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ..., -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.111111111111\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "# tf.reduce_mean; Computes the mean of elements across dimensions of a tensor.\n",
    "# If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned.\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393812],\n",
       "       [-0.04269557],\n",
       "       [-0.66145277],\n",
       "       [-0.63752776]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above except for the gradients = ... line:\n",
    "\n",
    "The gradients() function takes an op (in this case mse) and a list of variables (in this case just theta), and it creates a list of ops (one per variable) to compute the gradients of the op with regards to each variable. So the gradients node will compute the gradient vector of the MSE with regards to theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you find the partial derivatives of the following function with regards to a and b?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.212537\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply replace the preceding gradients = ... and training_op = ... lines with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use a different optimizer, we just need to change one line. For example, we can use a momentum optimizer (which often converges much faster than Gradient Dscent) by defining the optimizer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.530564\n",
      "Epoch 200 MSE = 0.525011\n",
      "Epoch 300 MSE = 0.524411\n",
      "Epoch 400 MSE = 0.524333\n",
      "Epoch 500 MSE = 0.524322\n",
      "Epoch 600 MSE = 0.524321\n",
      "Epoch 700 MSE = 0.524321\n",
      "Epoch 800 MSE = 0.524321\n",
      "Epoch 900 MSE = 0.524321\n",
      "Best theta:\n",
      "[[ 2.06855798]\n",
      " [ 0.82962859]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.30571091]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size)) # np.ceil; Return the ceiling of the input, element-wise. \n",
    "                                         # The ceil of the scalar x is the smallest integer i, such that i >= x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save and restore a model, just create a Saver node at the end of the construction phase (after all variable nodes are created); then, in the execution phase, just call its save() method whenever we want to save the model, passing it the session and path of the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")  \n",
    "error = y_pred - y                                       \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145277],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoring a model is just as easy: we can create a Saver at the end of the construction phase just like before, but then at the beginning of the execution phase, instead of initializing the variables using the init node, we call the restore() method of the Saver object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have a saver that loads and restores theta with a different name, such as \"weights\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension .meta. You can use the function tf.train.import_meta_graph() to restore the graph structure. This function loads the graph into the default graph and returns a Saver that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when we keep tweaking and saving our model: we can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a node in the graph that will evaluate the MSE value and write it to a TensorBoard-compatible binary log string called a summary. The second line creates a FileWriter that we will use to write summries to logfiles in the log directory. The first parameter indicates the path of the log directory (in this case something liket tf_logs/run-20170906091959/, relative to the current directory). The second (optional) parameter is the graph we want to visualize. Upon creation, the FileWriter creates the log directory if it does not already exist (and its parent directories if needed), and writes the graph definition in a binary logfile called an events file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to update the execution phase to evaluat ethe mse_summary node regularly during training (e.g., every 10 mini-batches). This will output a summary that we can then write to the events file using the file_writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07033372]\n",
      " [ 0.86371452]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.80304712]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, 0., name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a threshold variable the classic way, by defining it outside of the relu() function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1 \n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow offers another option. The idea is to use the get_variable() function to create the shared variable if it does not exist yet, or reuse it if it already exists. The desired behavior (creating or reusing) is controlled by an attributeof the current variable_scope(). For example, the following code will create a variable named \"relu/threshold\" (as a scalar, since shape=(), and using 0.0 as the initial value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the variable has already been created by an earlier call to get_variable(), this code will raise an exception. This behavior prevents reusing variables by mistake. If we want to reuse a variable, we need to explicitly say so by setting the variable scope's reuse attribute to True (in which case we don't have to specify the shape or the initializer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will fetch the existing \"relu/threshold\" variable, or raise an exception if it does not exist or if it was not created using get_variable(). Alternatively, we can set the reuse attribute to True inside the block by calling the scope's reuse_variables() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once reuse is set to True, it cannot be set back to False within the block. Moreover, if we define other variable scopes inside this one, they will automatically inherit reuse=True. Lastly, only variables created by get_variable() can be reused this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somewhat unfortunate that the threshold variable must be defined outside the relu() function, where all the rest of the ReLU code resides. To fix this, the following code creates the threshold variable within the relu() function upon the first call, then reuses it in subsequent calls. Now the relu() function does not have to worry about name scopes or variable sharing: it just calls get_variable(), which will create or reuse the threshold variable (it does not need to know which is the case). The rest of the code calls relu() five times, making sure to set reuse=False on the first call, and reuse=True for the other calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first variable_scope() block first creates the shared variable x0, named my_scope/x. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables x1 and x2 have a name with a prefix my_scope/. Note however that TensorFlow makes their names unique by adding an index: my_scope/x_1 and my_scope/x_2.\n",
    "\n",
    "The second variable_scope() block reuses the shared variables in scope my_scope, which is why x0 is x3. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (my_scope_1) and thus the variable x4 is named my_scope_1/x.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable my_scope/x by creating a variable_scope() at the root scope (whose name is an empty string), then calling get_variable() with the full name of the shared variable (i.e. \"my_scope/x\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some café?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = x² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical differentiation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.000400000048216\n",
      "df/dy(3,4) = 10.000000000047748\n"
     ]
    }
   ],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.0\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # x² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic differentiation (autodiff) – forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}ε\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0ε"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4ε)\\times(5 + 7ε) = 3 \\times 5 + 3 \\times 7ε + 4ε \\times 5 + 4ε \\times 7ε = 15 + 21ε + 20ε + 28ε^2 = 15 + 41ε + 28 \\times 0 = 15 + 41ε$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0ε"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + ε\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + ε\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autodiff – Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n",
      "df_dx = 24.0\n",
      "df_dy = 10.0\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autodiff – reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's make_moons() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFKCAYAAADmCN3IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAlygiAIgjAxJMgJgiAIwsT8f7bCr3h29ZY8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1225295f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $\\mathbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reshape y_train to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moons dataset has two input features, since each instance is a point on a plane (i.e., 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. This model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\mathbf{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T \\cdot \\mathbf{x})$\n",
    "\n",
    "Recall that $\\mathbf{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, TensorFlow has a nice function tf.sigmoid() that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\mathbf{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-7  # to avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's tf.losses.log_loss() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(y, y_proba)  # uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.792602\n",
      "Epoch: 100 \tLoss: 0.343463\n",
      "Epoch: 200 \tLoss: 0.30754\n",
      "Epoch: 300 \tLoss: 0.292889\n",
      "Epoch: 400 \tLoss: 0.285336\n",
      "Epoch: 500 \tLoss: 0.280478\n",
      "Epoch: 600 \tLoss: 0.278083\n",
      "Epoch: 700 \tLoss: 0.276154\n",
      "Epoch: 800 \tLoss: 0.27552\n",
      "Epoch: 900 \tLoss: 0.274912\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single for loop rather than 2 nested for loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set).\n",
    "For each instance in the test set, y_proba_val contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54895616],\n",
       "       [ 0.70724374],\n",
       "       [ 0.51900256],\n",
       "       [ 0.9911136 ],\n",
       "       [ 0.50859052]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]], dtype=bool)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, we may want to choose a different threshold than 0.5: make it higher if we want high precision (but lower recall), and make it lower if you want high recall (but lower precision).\n",
    "\n",
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86274509803921573"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88888888888888884"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFKCAYAAADmCN3IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X14VNW9L/DvTIYQQjCGGElOCY31BFIgWMHrxcK5KBEB\naYggCjkXerRYBISI2ioIIuViAvJqoBDKm4BIqkGIQSoVj+E0tVqLNEQjEqXhxZAQTAATDEkm+/6B\nM2Qye2b2zN57Zr98P8/j8zh79kzWmj3Mb6+1fmstiyAIAoiIiEiXrKEuABEREQWOgZyIiEjHGMiJ\niIh0jIGciIhIxxjIiYiIdIyBnIiISMdsoS6Av2prvwt1EfwSExOJ+voroS5GyJi5/qw76242Zq47\noG794+K6eXyOLXKV2WxhoS5CSJm5/qy7ObHu5hWq+jOQExER6RgDORERkY4xkBMREekYAzkREZGO\nMZATERHpGAM5ERGRjjGQExER6RgDORERacreigIMy78LCRtiMCz/LuytKJD9np9++g/84hcjMGvW\nNMye/TimTXsEBQX5fr3H88//FgDw9ddf4Z///BQA8OKL89DS0iK7fHLobmU3It34+GOE/+ssmtPu\nC3VJiHRjb0UBHn/vV87HX9R97nw8LnmCrPceNOgO/O53OQCA5uZm/Od/PoiRI8egWzfPq6a1l529\nHABQXPw+YmNj8bOfDXS+XygxkBOpJTcXXf9xBM3DhgM2/lMjkmLNkZWix1/5dJXsQN7elStXYLVa\ncfLkV8jLW4ewsDCEh4fj2WcXICYmBgsXzkVjYyOampowbdpM3HnnYIwdOxJbtuzEn/60HzZbJ/Tu\nnYKFC+dhx458PPro/8X+/UUAgNdf34mwMCvuvjsNL7+cjatXm9C5cwSeffZ59OgRr1gdHPjrQqQC\na0018OabsLW0IOLVzWh6bHqoi0SkCyfqj/t13B9HjvwDs2ZNg9Vqhc1mw1NP/Ra5uaswd+4CJCf3\nwV/+Uox161bhV796HJcuXcLKlbmor6/HmTOnnO8RF3czRo/+BWJjY9G3b38AQFiYDcOGDcef//xn\nDBmShkOH3sXq1b/HypXLMGHCRNx11xD84x9/R17eOrz44hLZ9eiIgZxIBRHbtwI/jJt1XZ6Dqw8+\nDCGmu1/vYTvyCawX69k1T6bSOyYFX9R9LnpcrvZd6w5Lly5BcnIfAMBttw1EXt46/OQntyIjYzwW\nLZqP1tZWTJgwyed7p6c/gNzc5YiJ6YHExB8jOvpGnDz5FXbu3IZdu7YDuBbw1cBATqS05mZE7Njm\nfGitr0fk8hw0/jC+JlWXTXmwfXaMXfNkKnMGPeMyRu7w5MCnVfl7N90Uh6++qsC//3sy/vnPT5GY\n2Atff/0VrlxpxPLlr+DChQuYMeNXGDLkP5yvsVqtaGsTXN4nMbEXBEHA66/vxLhx14YAevVKQmbm\nZKSm3oZTpypx9OgRVerAXwcihXUufAth52tcjnV5dQuaHv017Mm9Jb2HtaYanYv2wcKueTIZxzj4\nK5+uwon64+gdk4InBz6t6Ph4e889Nx+rV78MQRAQFhaGuXNfwE03xWHbtj/gv//7ENra2jB16uMu\nr+nT56dYv/4VJCXd4nJ8woQJWL16DQYOvAMA8MQTT2LlyqVobm7G1atNePLJ36hSB4sgCILv07RD\nb/uRx8V1012ZlWTG+t846h50+tT9zvtq2ghc3r1H0ntEvpyNriuWAgDaYmJQ99FRv7vmQ8mM192B\ndTdn3QF168/9yImC6OK7H6D2/GVAEFB7/jKaxj+E1t59cHnnH6W9gYeueSIiMQzkRCpydJHbTnyJ\niFc3S3qNp675sIoTahSRiHSOgZxIRRHbt8LSLnvdUl/n8rztyCcIf//PLse6bNno9j6W1lZ0XThP\nvYISkW4x2Y1ILV6y1x1Tyzq/+Ue3zPSL734QqhITkQ6xRU6kljfe8NhF3mVTHrrOfw6d397rV7e7\nN2KteyIyPgZyIrWsXet2yNLaiqjnnr42bn7ya1haWwGId7v7q8umPHR9cT7ww3sSkTkwkBOp5eOP\nUXv+stt/LYN/7hw3d5CbmR5IUh2Rlindw/Tpp//AyJHDUFNT7Ty2YcNaHDhQJPu9r169iqKifQCA\nAweKUFJyWPZ7+oOBnCiYOoybtycnM91XUh2R3qjRw9SpUziysxdD6eVT6uq+dQby++9Px9ChwxR9\nf1+Y7EYeca3vwNmOfAIIV4E7hrocF5ta5uDITJe6aIyTQkvCEmmFWisbDhp0B9raBLz11ht48MGJ\nzuMFBfl4772DsFgsSEu7Dw89NAlnz57BSy8tgs1mQ3x8As6dq8K6dX/Anj1/xOHDH+D777/HjTfe\niOzsFdixYysqK/+FdevWoaGhCbGxsThz5jT+/d97Y/ToX+Dbby/gt7+dg61bX0Ne3jqUlh5FW1sb\nJk78vxg+/F7Z9WKLnDzimGvgumzKA555xu2zE5taBlxb9a32/GX/gzg475yMR80ept/8Zi7++MfX\ncfbsGQBAU1MT3n//Paxfvxm///0m/OUvxTh9uhK///0r+OUvH8XatRuRmnobAKCtrQ2XLl3CmjXr\nsWnTdtjtdnzxxef45S9/haSkWzBr1izn3/nFLx7An/60HwBw8OABjBmTjr/97a84d+4bbNiwBbm5\nedixYyu++07+SnCyA7kgCJg7dy62bNki+nxxcTHS09MxcuRIZGVloaGhAQBgt9uxZMkSjBo1CiNG\njMDu3bvlFoUUxDHXwDk+O3zxhdtn51j1reN/gQRwB847J0NReWXD6OgbkZX1DF566UUIQhu+//4K\namqq8eSTM/DkkzNw6dIlnDlzBqdO/Qv9+18L4Lfddvu1slit6NSpExYtmo+cnMU4f/48Wj00dG65\n5Sew2+2orj6H999/D/fddz9OnvwKX355HLNmTcMzz8xGa2srqqurZNdJViD/+uuv8V//9V/405/+\nJPp8XV0d5s2bh7Vr1+LgwYNITEzEihUrAAD5+fk4deoU9u/fj4KCAmzfvh3Hjh2TUxxSEMdcAxfs\nz+7iux+g8TdznY/bYmJw4ctKWTcHRKESjB6moUP/DxITf4wDB/ajU6dwJCX9BGvXbsS6dX/A/ff/\nArfemoyf/ORWfPbZtZj0+edlAICvvqrA//xPMRYvzsFTTz0LQWgDAFgsVuf/t/eLX2Rg/fpcJCXd\ngm7duuHHP07C7bffgXXr/oDc3DwMH34vfvSjnrLrIyuQ79q1C+PHj8fo0aNFny8pKUFqaiqSkpIA\nAJmZmSgqKoIgCDh06BDGjx8Pm82G6OhojBkzBm+//bac4pBSuNZ34ELx2fF6kYEEq4fpySefQefO\nnREVFYU77vhfmDlzKqZOnYIzZ84gLi4OM2ZkYdeu7XjyyRkoKfkf2Gw29OyZiC5dumDGjF/hqadm\nIjb2Jly4UIuYmBi0tLRi+XLXvJR77rkXf//735Ce/gAAYMiQ/4PIyC6YOfMxTJ06GRaLBZGRXWXX\nRVay28KFCwEAH330kejz1dXViI+Pdz6Oj49HQ0MDGhsbce7cOSQkJLg89+WXX8opDilEiW04HYly\nmPSgGkXUHEd9LXV1sj87fylxvYi0Qq2VDQcOvMO5vSgAdO0ahT179jsf/+d//tLl/M8/L8PcuS+g\nZ89EFBXtQ1lZKSIiIpCbmyf6/q+++rrb7mcRERF4991i52OLxYLZs5XfV13VrPW2NveuBuDaOINY\n+r/V6ruDICYmEjZbmOyyBZO37ec0abv7mLiltRXdl7wAHDgg7T12bgGOHgUmZGiz/h9/DNTVAR56\nk/zmqG9UlNtTfn92/vJ2vV58Udl6+kGT1z1IWHf96937Fvy//7cAXbp0gdVqRXZ2tqS6haL+qgby\nhIQElJaWOh/X1NQgOjoakZGRSEhIQG1trctz7VvvntTXX1GlrGrR5f68+w95fk5CXaw11ej+5pvX\nxok3bEDtpEc0N5Wt28srYfvsGOp/Nti5xnmg2tf3u+yX0fTO+wBErr1a3wMv16vb9KmK1dMfuvze\nK4R1N0bdk5JSsHHjdpdjvupmyP3Ihw4ditLSUlRWVgK4luCWlpYGAEhLS8OePXvQ2tqKy5cv4513\n3sG998qfT0eh1z7ZC4sWwVJfp6mpbEpn5Gs1MZAzD4jMQfFAXlZWhoyMDABAbGwscnJykJWVhdGj\nR+PEiRN47rnnAFxLfEtMTERGRgYmTJiACRMm4M4771S6OBRsHVcuq6tD5O8WaCqgKBp4NZxoptUb\nDCJSlkVQeq06lemt28ZIXU1SdH4zHzc8Mc3lmGCxwPLD16wtJgZ1Hx2FENM9FMUDmpvRfWA/l+Sw\nK489HvAqaKL1tdlQf/gjdP/5oNBde4Xr6S+zfe/bY93NWXfAoF3rZD6iU0fa3SuGusWq1BxVx4YO\nWl2Mhau9EZkHAzkpquPKZdi50+2cUAYUpQKvY8z/4v73fK7UFop9wtW+weDe50TawU1TSF0e9uT2\ntTmIWlnuSsxR9XdDhy6b8mD77Biahw0PWua4WnNxHUJRJyISxxY5qcvDnty+lg/VUpZ7R/4kkYUq\nc1zNFjOz4Ym0hYGcNCfYgcKvoOdnlnqoMsfVvBGKXLqE2fBEGsJATpoT7ODnT9DzK4ksRFPTVL0R\nam5GRMEb1/+WhqbbEZkVAzlpS5CDn79Bz68ksjfeCEnmuJQboUC73iN2bIPlapPLMU91av83mBxH\npB4GctIUTy3e8H17VAkE/rb+/dpP3Euin2ok3ggF2vUembvK7ZinOrX/G1rOeSDSOwZy0hRPLd6o\nF+YpHwjUbv0HmOgnh5Su/4C73pubgQ4bIV157HHxOp075/wbkbmrmBxHpCIGctIUsRbvt2UnYK37\nVvFAYMRFU6R0/Qeag+DX57Vxo/NvRK5ZweQ4IhUxkJPfHOOdwRr3VCv5Taurssnhs+tfRi+E5M+r\nuRnYeP1cS9P1MXUmxxEpjys5kN8ci4G09kuF7fMydRcF8RB4lFgzXO1FU7TIU6u66dFfw57c2+tr\npX5enQvfAqqrPT4v9e8RkTRskZNf2o+vdn57r+rjnkbs/g6lYPRCiP0NNf8ekdmxRU5+ad/NbbHb\nAVzr7r764MOq7GjmLfComTRmVMHohbj47geSd4FSayleIjNhICfpOu41/gNrfT2ifjsHVzMna3Jt\ndNIurtlOJB+71kkysW5u53NFhYia91vOEybJxKbBceEYIv8xkJNk3sY+LYKAsMp/cZ6wzoQycIrN\nRuDCMUT+YyAnyTxNbWr8zVznOZwnrC8hC5wisxG6/u4FLhxDFAAGcpInRBuDGF0wWsqh3I5UbJgm\nIn8XF44hCgADOcnic3rYxx9zzDMAwWgph2qLVcDDbIR2y7/yhpBIOgZyndFSMpDtyCeIXPWy23GX\necK5uRzz9FNQWsoh7km5+O4HqP/T+7i0u+DaynO//4PbOVwvgEgaBnKd0VIyUJdNeUBYGGqr6kSX\nBLXWVANvvskxTz/JbSlLudnTwkI7LrujGXC5XKJgYSDXkVCOaQZSlojtWwGOefpHgZaylJu9UAfO\njt8fv7aHJSIXDOQ6EsoxTb/LwiS4gMhtKUu92Qt14FTqu6yloSaiUGEg14sQBEaPP5ISyqKFrls9\nkttS1tLNnkd+fpe9BWstDTURhQoDuU6oERh9tWY8/UhKKUuou271SlZLWSe9IP5+lz19D7U01EQU\nSgzkOqFGYPTWmvH2IymlLI6ABEHgmGeQ6KUXxJ/vsrfvoS56H4iCQPYuBcXFxVi5ciWam5vRp08f\nZGdnIyoqyvn8vn37sG3b9VbCd999h5qaGhw+fBg33XQTBg8ejB49ejifnzp1KsaOHSu3WIaj9OYh\njh9IS0sLIl7djKbHprs83/FHsv3uZtzIRJv0slOc4/vTbfpU2D47hvriv3ncMMXj91DFfeqJ9MYi\nCIIQ6Ivr6uowZswY7N69G0lJSVi+fDkaGxuxaNEi0fNbWlowefJkjBs3DpMmTcLJkycxY8YMHDx4\nUPLflLI1opZI3c4x2CJfzkbXFUsBAG0xMaj76Oj1bUibm9F9YD+X1t2Vxx4P6EdSq/UPBtbdc92t\nNdXoPrAfLC0t+C77ZbcbSQBev4ed38zHDU9MczldsNlQf/gj2JN7K1aPQPC6m7PugLr1j4vr5vE5\nWV3rJSUlSE1NRVJSEgAgMzMTRUVF8HRvsGnTJnTv3h2TJk0CABw9ehRWqxVTpkxBeno61q1bB/sP\ne1yTinyMpeqli5b0S0q3uLfvIXMwiK6TFcirq6sRHx/vfBwfH4+GhgY0Nja6nVtXV4dt27bh+eef\ndx6z2+0YMmQItmzZgl27dqGkpAQ7d+6UUySSwFeg5o8kqUpiUp6372Gop88RaYmsrvW8vDxUVVVh\n8eLFAIDW1lb069cPR48eRWRkpNu5lZWVWLp0qcf3O3jwIHbu3InXXnvN4zmtrXbYbGGBFpkA4H//\nb+Dvf3c/Pno0cOBA8MtD5vLaa8CUKa7HbDagrAxISZH//h9/DNTVXfs+E5mArGS3hIQElJaWOh/X\n1NQgOjraLYgDwIEDB7BgwQKXY/v27UNKSgpSfvjHKwgCbB6SXhzq66/IKXLQaXLMaP8hz88pXFZN\n1j9IWHfxut+4eg06dTzY2oqrs7IUaVF3e3nltSS6nw32mESnJl53c9Yd0OkY+dChQ1FaWorKykoA\nQH5+PtLS0tzOu3TpEk6fPo3bb7/d5XhFRQVyc3Nht9vR1NSEXbt24f7775dTJCLSODW7xTm3nMxI\nViCPjY1FTk4OsrKyMHr0aJw4cQLPPfccysrKkJGR4Tzv1KlTiIuLQ6dOrvfhs2bNQnR0NNLT0zF2\n7FjcfvvteOihh+QUiYhMjHPLyYxkjZGHgt66bdjVZN76s+5BrruC0ybl4HU3Z90BnXatk/5x0wky\nCk6bJLNiIDc5bjpBeiR2A8ppk2RWwU/pJM3wtUwrkVZ12ZQH22fH0DxsuDMznUsHk1mxRW5iTAwi\nPWJmOpErBnKz0smWl0Qd8QaUyBUDuUkxMYh0iTegRG4YyE2KiUGkR1q6AeWMD9IKBnKTuvjuB/i2\n7ASEHxbp+S77ZW46QZqnpRtQzvggrWDWukHZjnwC68V6NKfd5/GcjmONVx98+Pqe5EQapJXMdM74\nIC1hi9ygfLYWONZIFDAm3JGWMJAbkJTpOVoaayTSFd4Ek8YwkBuQlNaClsYaibzRWlIZb4JJaxjI\njUZia0HNrSSJlCQ3qUzpGwGfN8Eff6ypGw8yPia7GYyn1kLTo7+GPbl3iEpFFBglksrElnOVw2fC\nXW4uuv7jiGJ/j8gXtsgNhl3mZCRyk8qCvZyrtaYaePNNLh9LQcVAbjDsMifDUCCpzOVGIHux6tnl\nEdu3AsxmpyBjIDcArSUDESlBdlJZxxuBhgZELstWsoje/x6z2SlIGMgNgCtMkRFJGSbydhMrfiOw\nWbXscmazU6gwkOuctzFAttRJz6QME3m7iRW9EWhrQ9S836hSXuanUKgwkOuct2QgXy11BnrSM1+J\nbBff/QC1Zy/AfnMPl+Otvfs4/1/JfwOOGw8IAvNTKKgYyPXMy5iclGxddsmTnknJaPfV3c1/A2QE\nDOQ65u1HytePXLCn5ZB07CmRQOQmNuq3T7l9bt66u/lvgIyCgVzHPP5ILXjOZ/YsN33QrvatRAZ1\ncWI3sZ2L9qHrvN+4tK69jbPz3wAZBQO5jnn6kbo6YaL37FlOk9Gsjq1Edv2KE72JFQTYKiulta75\nb4AMhIFcJi22mHxlz3KajHa5tBKXvcSuXw863sQ2/mau8zkprWv+GyAjYSCXSYstJl/TdjhNRqM6\nthIvXWLXrxR+tK4dN96+/g1o8QadyBOu6C+DpA0dPv4Y4f86i+a0+2T9LduRT2C9WC/7fQAJmz5Q\nSIi1Eh0cwakxe3mQS6V9/mwU5NhApb74b143NFF6oxUiNbFFLoOkZJnc3IBa7B1bBGItfzmtBrY4\ntEeslejyPLt+RUntYZKapc5sdtIb2YG8uLgY6enpGDlyJLKystDQ0OB2ztKlS3H33XcjIyMDGRkZ\nmDNnDgDAbrdjyZIlGDVqFEaMGIHdu3fLLU7wSOjOk7MTUvvA7emHRU63vhaHBMyu/ZBIy8BBbs9z\n+EOc1I2CpGapM5ud9EZWIK+rq8O8efOwdu1aHDx4EImJiVixYoXbeUePHsWqVatQWFiIwsJCrFmz\nBgCQn5+PU6dOYf/+/SgoKMD27dtx7NgxOUUKGinJMoHuhNQxcIv9sMhpNbDFoX3cxU5hUsfRmc1O\nOiQrkJeUlCA1NRVJSUkAgMzMTBQVFUEQBOc5zc3NKC8vx9atWzF27FjMnj0bVVVVAIBDhw5h/Pjx\nsNlsiI6OxpgxY/D222/LKVLQ+OzOk/GD4BK4X85BxKtb3N5HTquBLQ4yG6lZ6kpns3MIi4JBViCv\nrq5GfHy883F8fDwaGhrQ2NjoPFZTU4PBgwfj6aefRmFhIW677TbMnDkTgiDg3LlzSEhIcHl9dXW1\nnCIFja8WU8A/CB1vAC7WI+xCrev7bNuMLts2XT/Hn1YDWxxkQlLH0ZWe0cEhLAoGWemYbW1toset\n1uv3B4mJidi06XrQmTp1KtavX4+zZ8+6tNzFXismJiYSNltYgCUOou3uXdaW1lZ0X/ICcOCA59e9\n9hrgIXPZ+T52OyzffutyLPLVLYh8Zg6QkuK9XCLvL/m1AYqL66bK++oB664RR/4hergzgLhAzvMh\nLq4bcO4cULQPaGlBXMFrwOzZfryDfmnquodAKOovK5AnJCSgtLTU+bimpgbR0dGIjIx0Hjt+/DiO\nHz+OBx54wHlMEAR06tQJCQkJqK2tdXl9+xa+mPr6K3KKHDz7DwG4dlFra79zfa7j43ZuXL0GnUSO\nX00b4Wzt3zjqHnT69IjrCa2tuDory+cYquj7S3xtIETrbxKsu7nrHrkqF11/GMJqe/FF1I0cCyGm\ne4hLpy4zX3dA3fp7u0GQFciHDh2KZcuWobKyEklJScjPz0daWprLOVarFS+99BIGDRqExMREvP76\n6+jTpw/i4+ORlpaGPXv24J577sGVK1fwzjvv4He/+52cIumelDnecuaBcw45URB4GMLiOgCkBlmB\nPDY2Fjk5OcjKykJLSwt69eqFZcuWoaysDAsWLEBhYSF69+6NBQsWYMaMGbDb7YiPj8eqVasAXEuO\nO336NDIyMtDS0oKJEyfizjvvVKRiRHqj5KI/5B+lP3t/FqkhkssiiA1Ua5jeum3Y1WTe+vtb927T\np0padUwP9Hbdlfzs4+K6oWXQHe7DX3AdIjMivV13pemya52IlCFpuV9ShRqfPYewKJi4RCuRn9SY\nG8y5/aHDz9489lYUYFj+XUjYEINh+Xdhb0VBqIukCAZyHeCiEtqi+Nxgzu0PHX72prG3ogCPv/cr\nfFH3OeyCHV/UfY7H3/uVIYI5A7kOcFEJ7VBjeVvujR06/OzNY82RlaLHX/l0VZBLojwGco3juuja\nokY3LPeHDx1+9uZxov64X8f1hMluGtcxcFx98GHDLyqhWSrNDWZiVOjwszeP3jEp+KLuc9HjescW\nuZZx/E5T2A1rbMxFMbY5g54RPX68rlz3iW8M5BrGwKEt7IY1NuaiGNu45AnYOGIrekb1dDneJrTp\nPvGNgVzDGDi0hXuEGxdzUczjbMNZj8/pNfGNY+QaxvE7ouBgLorx7a0oQNZ/z/B6jl4T39giJyJz\nYy6K4TnmkF+1X/V6Xu+YFF0uGsNATkSm5m8uCpPi9MfTHPKOfv5vQ0QXjbl9R19NB3YGciIyNX9z\nUZgUpz++usx7RvXExhFb8ddvSkSf/6bhrKZXg2MgJyJT8yeJUUpSXP5n+brrmjU6T3PFO4dFYOOI\nrfj0l+UYlzxB8hi51pLiTB/I2U1GRFL5Wtlvb0UBMvdkGnI9bz3zNIc8d/h6jEue4HwsdXEYrSXF\nmT6Qs5uMiCSRkBRn5PW89cwxh7xvbH/YrDb0je2PjSO2ugRxwHPA70hrq8GZevoZ94AmIqk8JcU1\nPfpr2JN7AzD2et56Ny55glvgFjsHuHbjdaL+OHpExuMbkXnnTw58WpUyBsrULXLuQ0yhwOGc0Atk\nipGUpDhPLTWtteBI3N6KAqw5shJf1n2B5Bv7YOFdiyW15EPNvC1ylTbAIPKly6Y82D47huZhwwGb\nef8JhopjTrGDYxwbgNcfaCkLNM0Z9IzLeztorQVH7jx9LzaO2IriiR+GsGS+mbZFznXMKRS4FGjo\nqTmOPS55AnY/uFvzLThyp+f8BtM2B7x1k3HtbFILlwINPX/GsR1drSfqj6N3TArmDHrGZ1Ce1H8S\n0nqMUaSsFDx6zm8wbSDnOuYUdBzO0QSp+1IH2gVP+qTn/cpN27VOFGwcztEGT1OMOo5j67mrlfwn\n9XuhRQzkREHCbWm1QeqcYj13tZqJUpucSP1eaJFpu9aJgo3DOdohZU6xnrtazULp4Q8p3wstYouc\niEiEnrtazYLDH9cwkAcZFwMh0gc9d7WaBYc/rpHdtV5cXIyVK1eiubkZffr0QXZ2NqKiolzOKSws\nxJYtW2CxWNClSxfMnz8fqampAIDBgwejR48eznOnTp2KsWPHyi2WZnExECL90GtXq1lw+OMaWS3y\nuro6zJs3D2vXrsXBgweRmJiIFStWuJxz8uRJLF++HJs3b0ZhYSFmzJiB2bNnO5+Ljo5GYWGh8z8j\nB3EuBkJEpBwOf1wjK5CXlJQgNTUVSUlJAIDMzEwUFRVBEATnOeHh4ViyZAluvvlmAED//v1x4cIF\nNDc34+jRo7BarZgyZQrS09Oxbt062O12OUXSNK7tTqQ/SmVFq/2eZsThj2tk9e1WV1cjPj7e+Tg+\nPh4NDQ1obGx0dq/37NkTPXv2BAAIgoCcnBwMHz4c4eHhsNvtGDJkCJ599lk0NTVh2rRpiIqKwiOP\nPCKnWNrExUCIdEeNRWG40IyyOPwBWIT2zWc/5eXloaqqCosXLwYAtLa2ol+/fjh69CgiIyNdzr1y\n5Qrmzp1GtOsIAAAgAElEQVSL6upqbN68GTfccIPb+x08eBA7d+7Ea6+95vFvtrbaYbOFBVrk0Hnt\nNWDKFNdjNhtQVgakmGs8h0gvBmwYgLLzZe7HewxA6fRSzbwnmZusFnlCQgJKS69/8WpqahAdHe0W\nxKuqqjB9+nTceuut2LFjByIiIgAA+/btQ0pKClJ+CGSCIMDmIwGsvv6KnCIHXVxcN9TWfocbV69B\np45Ptrbi6qwsQ6/t7qi/GbHu+q97eW25x+Oe6uer7oG8p14Y5boHSs36x8V18/icrDHyoUOHorS0\nFJWVlQCA/Px8pKWluZxz8eJFTJ48Gffddx9Wr17tDOIAUFFRgdzcXNjtdjQ1NWHXrl24//775RRJ\nsy6++wFqz192+09uEOd0NiL1qLG/OPcsJ6XJCuSxsbHIyclBVlYWRo8ejRMnTuC5555DWVkZMjIy\nAAC7d+/GuXPn8N577yEjI8P5X319PWbNmoXo6Gikp6dj7NixuP322/HQQw8pUjEtUzL4dtmUh64v\nzgdaWxV5PyK6To2saGZak9JkjZGHgt66bcS6WrpNnwrbZ8dQX/w3WXPJrTXV6D6wHywtLfgu+2U0\nPTZdbnEVZ+auNtbdGHXfW1GAVz5d5dzK9MmBT3tNrpJSd3/fUy+MdN0DEaquda5IEiS2I5/AerEe\nrf0HoHPRPlhaWhDx6mZZwZd7WxOpT42saGZak5K4RGuQOLrAI7ZtVmYuuYfpbKQhH3/s9xAKcx6I\nyF8M5EHQfkW3Ln/YcP24jODLva11IDfX7/wF5jyYCxeGISUwkAdB+y5wa4Pr+EmgwZd7W2ubtaYa\nePNNv5bjDX/vIDoXvsUlfE3CsTDMF3Wfwy7YnQvDMJiTvxjI1dahC7wjS2srus38td9vq9Z0NlJG\nxPatgJ9DKF2XvAjLD0sUcwlf7VCr1cwtOEkpDORqe+MNty5wwWZD3V//gdrzl9E0/iFYvr/CrlQj\nCSB/wXrmNMK+uL5QCHMetMGfVrMj4NsW2yQFfG7BSUphIFfb2rVuhxxd4NwNzTjaJ6kFkr8Q9cJc\nWDocY85D6EltNQfSTc6FYUgpDORq+/hjj13g3A3NONonqfmdv9DcjPA/v+vfaygopLaaA+km58Iw\npBTOIw8B25FPYL1wnruhGYQjSc1ityPi1c24+O4HAK4t4ND04MM+F//pXPgWLB2GVgSbDfWHP4I9\nubfq5SfPesek4Iu6z0WPtxdIN7ljHrkRF4ah4GKLPAQil72EqKyZnD5mEGJJansrCjB86U9hLbyW\nuX7i5VkeX88ZCNoltdUcaDf5uOQJKJ74Iaqm16F44ocM4hQQtsiDzFpTjfDDH8AisjKu48f7ym/m\nwnqxHs1p94WghOQPsSS16vlT8Xj/9/HiB0D4tfiOlI2vY//Iwbh/0CNu7+FowZP2SG01D/nRUNGW\n+8//bUhQyknmxkAeZJEvPu8M4m3h4agrO+G2rKpjLfbmYcNlrcVO6hNLUuvz1vvofxPw+JHrx2K/\nB8KWLgLefMTtPfZWFGDNkZXOQDFn0DNsmWmIlOVU//pNiejxD6v+qkaRiFywaz2YmpsRUbjX+dDa\n3IzIxQtdTmEmu454SFLr1Abs3AskNLgen/iXOrehEy4KYgycSkahxEAeRBFb/+AcS3XosmuHy487\nM9n1QyxJrcUKpDwBXBXpSOnUBrdxby4KYgycSkahxEAeRF1XLnM7ZgEQ9eSMaw+4EYquiCWpdWoD\nVh0EBv8asCxy/e8Pf93qtvIeW3L64Gt1N09JcZeuXuQ66qQ6BvJgaW6GpbFR9Kmw6moA0hYS4e5Y\n2iG2TG7f3H4YM9n93B9F9RQdZ5XSkuPGGqElZfhjXPIEbByxFX1j+8NmteFHUT0BAN80nOWQCamO\ngTxIxLphHazV5xBWcULSNCTujqVtnlpmC+9a7Nf5julNHEMPPanDH46pZC0vtOCG8GhJryFSAlOi\ng0QsSDs4grWvaUiORDhLSwsiXt2MpsemK11MksnfRT58ne8tiDCzPTgCGf7gkAkFEwN5kDiCtLWm\nGt0H9oOlpQXfZb/sVzDumAh39cGH3aauUeg5pivFxXVDbe13ks8Xw4AQelJXd5P7GqJAsWs9yALO\nSmcinCkxGzr0AlkTneuoUzAxkAeTjGAcyI5apF+OBLfjdeWizzMgBE/HRLa+sf2xccRWr0MbgbyG\nKFAWQRBZK1TDpHRVakn77tXOb+bjhiemuTwvdXOMG0fdg06fHnE7fjVthNuUJi2R2r1sRIHW3ZHg\n1pHVEoaU7j/VxcYavO6su5aptZqimvWPi+vm8TmOkQeRt6x0X8GY63Gbh6cEt5TuP0XxxA+DXBoi\nY+l4o+yYCQJA8zfInjCQBxGDMUkRSIIb12snksaIM0E4Rk6kMd4S3MQWh/E01/z2HX0535yoAyPO\nBGEgJ9IYTxnPP/+3IaIBe/HfFoqe/03DWS4eQ9SBEWeCMJATaYynjGdPW2V+03DW6/txNTGi69Se\nGhiKJZU5Rk6kQWKLxMw89OuA3kvPXYZESvN39UV/5H+WH5JEOtkt8uLiYqSnp2PkyJHIyspCQ0OD\n5HPsdjuWLFmCUaNGYcSIEdi9e7fc4hAZlqeuP8cGHZ7ER8arURwi3XKsi181vQ7FEz9ULMhm/yVb\n9LjavWKyAnldXR3mzZuHtWvX4uDBg0hMTMSKFSskn5Ofn49Tp05h//79KCgowPbt23Hs2DE5RSIy\npL0VBbjcfEn0uYV3LcbGEVths4h3sOlqoQidat+dOmDDAOYl6JicrvHyWvEFnNTuFZMVyEtKSpCa\nmoqkpCQAQGZmJoqKitB+jRlv5xw6dAjjx4+HzWZDdHQ0xowZg7fffltOkYgMx5GV3nEs/EdRPZ2r\nhY1LngDBQ8iuuVIdjGKaVsdZA2Xny5hkqFNydxvsG9dX9LjaiXSyxsirq6sRH3+92y4+Ph4NDQ1o\nbGxEVFSUz3POnTuHhIQEl+e+/PJLr38zJiYSNluYnGIHnbcVeczAzPVXou7rClaLHo/t2h3Tfv6o\n83HfuL4oO1/mdl7fuL4huQZmue6ers/vj61xuT5moefrLvdaPv8fzyNzT6bb8Rfunq/q5yIrkLe1\ntYket1qtks4RWx22/WvF1Ndf8aOEoaeXJQvVYub6K1V3T9115bXlLu8/67anRJd2fWLAnKBfAzNd\nd6nXxwz0ft3lXstJ/Sfh8uXv3RLp0nqMkf25qLZEa0JCAkpLS52Pa2pqEB0djcjISEnnJCQkoLa2\n1uW59q13IpK+Jaaa2bjkGbcsNQ4lrqW3bYnVImuMfOjQoSgtLUVlZSWAa8lraWlpks9JS0vDnj17\n0NraisuXL+Odd97BvffeK6dIRIbjz7xXtbJxyTNuWWocer2WslrksbGxyMnJQVZWFlpaWtCrVy8s\nW7YMZWVlWLBgAQoLCz2eA1xLfDt9+jQyMjLQ0tKCiRMn4s4771SkYkRGwZa2tnW8Pn3j+uKJAXN4\nfXRIr//WuI2pyvQ+ZiSXmevfse5a3thE6bLxurPuZsRtTIkMTMtbJ2q5bETkG9daJwoCb1snhpqW\ny0ZEvjGQEwWBlrdO1HLZiMg3BnIvbEc+Qfj7fw51McgAtLx1opbLRkS+MZB70WVTHrq+OB9obQ11\nUUjntDytRctlIyLfGMg9sNZUo3PRPthOfImIVzeHujikc572GNdCMpmWy0ZEvjFr3YOI7VthaWkB\nAHRdnoOrDz4MIaZ7iEtFehaKFZ+k0nLZiMg7tsjFNDcjYsc250NrfT0il+eInuoYR+d4OhERhQJb\n5CI6F76FsPM1Lse6vLoFTY/+Gvbk3q7HN+XB9tkxtPZLhe3zMjQPGw7Y+LESEVFwsEUuosuWjW7H\nLK2t6Lpwnsux9uPond/ey/F0IiIKOjYdRVx89wNJ57UfR7fY7QA4nk5EpCVaXhpZKWyRB6rDOLqD\nt/F0ItKmvRUFGJZ/FxI2xGBY/l3YW1EQ6iKRAhzLD39R9znsgt25/LDRri8DeYDExtEdury6BWEV\nJ4JcIqLAmTmQmeXHXg+U/h6aZflhBvIAiY2jO4iNpxNpldkDmVl+7LVOje+hWZYf5hh5gKSOoxNp\nnbdAZrSxRDFm+bHXOjW+h71jUvBF3eeix42ELXIikzN7IPP0o97a1mq6YYZQUuN76Gn54S++/dxQ\n15aBnMjkzL5piqcfewCmG2YIJTW+h+2XH7ZawpzHBQiGurYM5EQmZ/ZNU9r/2HtipvFypRLO/H0f\ntb6H45InoHjih+jj4YbACNeWgZzI5JTaNKX9D/eADQN01dJx/NiHtWu1tWeWYQalEs4CeZ+O38Mf\nRfXEj6J6YuahXyvSDW7kISQmuxGR7E1THD/cDmXny5yP9ZQwZ5bkKE+USjgL9H0c38OO3yfHjYDj\nnEAY+dqyRU5EshllCpfZhxmUarXKfR81vk9GvrYM5EQkm1G6Lc2+N7uchLO9FQUYsGEAEjbEwGYV\n7+yV2vpV4/tk5GvLrnUiks1I3ZZm3pt9zqBnXLq0HXy1Wjt2hdt/2HvC3/dxUOv7ZNRryxY5Eclm\n5G5LMwm01eqpK7xzWERArV9+n/zDFjkRuQhktyjH8698ugon6o+jb1xfPDFgjiFbP0YXSKvVU5e3\nXWhF1fS6gMoAXP8+9Y5JwZMDn+b3yQMGciJykpMt3D4AxMV1Q23td+oVlDRFja5wo3aDq4Fd60Tk\nZJTscwoudoWHlqwWeXFxMVauXInm5mb06dMH2dnZiIqKcjuvsLAQW7ZsgcViQZcuXTB//nykpqYC\nAAYPHowePXo4z506dSrGjh0rp1hEFCCjZJ9TcDlazr8/tgblteWKdYUHMsxjRgEH8rq6OsybNw+7\nd+9GUlISli9fjhUrVmDRokUu5508eRLLly/HW2+9hZtvvhmHDx/G7NmzUVxcjJMnTyI6OhqFhYVy\n60FECjBS9jkF17jkCZj280cVG1JRY1EYowq4a72kpASpqalISkoCAGRmZqKoqAiCILicFx4ejiVL\nluDmm28GAPTv3x8XLlxAc3Mzjh49CqvViilTpiA9PR3r1q3zOG2BiNTHLlLSCn+GeZRaH16vfLbI\nDx8+jBkzZrgdnzlzJuLj452P4+Pj0dDQgMbGRpfu9Z49e6Jnz54AAEEQkJOTg+HDhyM8PBx2ux1D\nhgzBs88+i6amJkybNg1RUVF45JFHFKgaEfmL2cKkFVKHedhyByxCxya0RHl5eaiqqsLixYsBAK2t\nrejXrx+OHj2KyMhIt/OvXLmCuXPnorq6Gps3b8YNN9zgds7Bgwexc+dOvPbaax7/bmurHTab+MYG\nRESkDfmf5SP7L9kory1H37i+eP4/nsek/pMkv37AhgEoO1/mfrzHAJROL/X7PCMLeIw8ISEBpaXX\nP6SamhpER0eLBvGqqipMnz4dt956K3bs2IGIiAgAwL59+5CSkoKUlGvjb4IgwGbzXqT6+iuBFjkk\nzD4Nx8z1N3Pd3695B4s/WGLKJCWzXve9FQVYV7oa5bXl6BGZgKrGs87nys6XIXNPJi5f/l7y92DW\nbU+JrjL3xIA5Lp9veW256OvLa8uDfh3UvPZxcd08PhfwGPnQoUNRWlqKyspKAEB+fj7S0tLczrt4\n8SImT56M++67D6tXr3YGcQCoqKhAbm4u7HY7mpqasGvXLtx///2BFomINGBvRQEy92TK3gqT9MPR\nvV12vgx2we4SxNtb/LeFuH1HX9y8/gbcvP4G/Gx7X4/fC6mrzMlZH94oAu5aB66Nn69cuRItLS3o\n1asXli1bhhtvvBFlZWVYsGABCgsLsWHDBuTm5qJ3794ur3311VcRERGBxYsXo7S0FK2trRg1ahSe\neuopWCwWj39Tb3e6Zr07dzBz/c1a92H5d4lmvveN7Y/iiR+GoETBZcbr7umaSyVn85KOY+Tt3xNA\nUKevhapFLiuQh4Le/oGY8R91e2auvx7rrsS83YQNMbAL7rNPbFZbQMt16o0er7tcnq65VHJv8vZW\nFLglaALwGODVCuahCuRcopXIwPwJzEpl/3Iuuvl4uuZSyV1wSGw512H5d4me+8qnqwyXr8ElWokM\nyhGYpY5VK7U8q1bmopt9bnEwebrmPaN6Ose3Yzp39/h6NW7yzLRKIQM5kUH5G5iV+uEblzwBux/c\n7fdWmEry9yZGD7R8Y+JITBvQY4DLNf/0l+Woml6H4okfIrKT+4wmBzVu8syUBMeudSKD8jcwK9kl\nPqn/JKT1GON2PFhrZ3u7idFjt6oeFj3xtURrdeM50eNWS5gqdZgz6BnRMXIjrlLIFjmRQfnbIvHU\nPfrzfxuiSHmC2UrWereqv61rPe9K56irp2S4lO4/VeXvSp2+ZgQM5EQG5e9Y9bjkCXgs9XG345vL\nNioSbIMZjLTcrRrIDY3Wb0w8aV9XT9RsIY9LnoDiiR86u/eNGMQBBnIiwwqkRfLXb0pEjysRbIMZ\njPy9iQnm+HMgNzRavjHxxlNdAfjVQvbn+mg5l0AtHCMnMjCxaTneSAm2gY5zB3Namj+bvwR7/DmQ\nGxq9jvd6qpPNapM8b9yf66OHXAI1sEVORE6+Wn5yxrnVmJbmrfUltVs12OPPgbSu5Y73hqqVqkRP\ngj/XR8+5BHIwkBORk69gK+eHUunkI6WS54I9/hzoDU2g472hnIqnxM2bP9dHr7kEcjGQE5GTr2Ar\n94dSyeQjpVpfwR5/DnY2dShbqUrU1Z/ro9dcArk4Rk5ELryNq2tp+VWlWl+hGH/2N3dBjlC3UuXW\n1Z/ro9dcArnYIiciybSy/CqgXOvL6PONPX0eYZYwXWR0+3N9jH4tPeHuZyoz405I7Zm5/katu9hO\nUx1/KINRd2/bV45LnhC0VeQ60tp19/Q5OSgZ6LRW92Dj7mdEpAtKdQvLDbTeppiZdRqSGEd9s/57\nBq7ar7o9r9dla+k6tshVxjtU89afdfdcd1+tabmG5d8lOpYvd99rKbR63YOxT7xW6x4soWqRc4yc\niIJO7UxqT4lcx+vKFXl/PTJrRrcZMJATUdCpnUntKTi1CW26SPBSg5YSFUlZDOREFHRqtw49BS3A\n+Kt8eaJkRrcZ1zPXMgZyIgo6T4H2YtNFRYLDuOQJsFrEf970sMqXWoFSiQV5QrlSHIljICeioOvY\nOuwZ1RMAUNV4VrHg0CdGfJ9rrY4JO4J3/IYbNR0opeQ3sMUeXAzkRBQS7VuH3cKjRc+R0w2upzHh\n9q3cNqFN9BytDAn4ym9giz34GMiJKOTUSH4blzwBj6U+js5hnQEAncM647HUxzU5Z9rbvt0OwRwS\n8Nai9pXfYNYdyEKJgZyIgkosSKiR/La3ogCbyzY6F0G5ar+KzWUbNdkylBKkgzUk4KtF7aunI9Rr\nu5sRAzkRBY2nIDHkR0NFz5fTDa6nlqGUIB2sIQFfn5uv7HfOVw8+BnIiChpPQeLDqr8qvtmFpxbg\nF99+rrlELG/T5XpG9Qzqxh9SWtTest/1lJtgFAzkRBQ03oKElKlR/mRDe2oBChA0l4jlaOU6svfb\nO9twVpW/6emzlNuiNusOZKHEQE5EQSMnSPibDe2tlduRFrrbxyVPUCV7X4y3z1KJFrUS89VJOlmB\nvLi4GOnp6Rg5ciSysrLQ0NAget7SpUtx9913IyMjAxkZGZgzZw4AwG63Y8mSJRg1ahRGjBiB3bt3\nyykOEWmcnCDh75i3t0VhOtJKIpancpR/+5miQwHePku2qPUn4G1M6+rqMG/ePOzevRtJSUlYvnw5\nVqxYgUWLFrmde/ToUaxatQoDBw50OZ6fn49Tp05h//79aGxsxMSJE9GvXz8MGDAg0GIRkYZ523rU\nl0CyofvE/FR0F7SOWttaMSz/Lo9bqQZrb/PeMSkey9u+5QzI247V12ep1Fa1FBwBt8hLSkqQmpqK\npKQkAEBmZiaKiorQcVfU5uZmlJeXY+vWrRg7dixmz56NqqoqAMChQ4cwfvx42Gw2REdHY8yYMXj7\n7bcDrw0RaV6g3a6BdMv7073uqas+mAucSC2v3K52ZpYbi88W+eHDhzFjxgy34zNnzkR8fLzzcXx8\nPBoaGtDY2IioqCjn8ZqaGgwePBhPP/00brnlFmzZsgUzZ87E3r17ce7cOSQkJLi8x5dffum1PDEx\nkbDZwiRVTiu87SNrBmauP+uunIX3LEDmnky34y/cPd/j35oW9yhuuKELckpyUF5bjr5xfTHsx8Nw\n+NRhHKs5Jvqa3x9bg2k/f9T5eF3BaknntRdo3TuWt7WtVfS8E/XHZX2+gXyWUpn5Ow+Epv4+A/mw\nYcNQXu6+h29eXp7o+VarayM/MTERmzZtcj6eOnUq1q9fj7Nnz7q13sVe31F9/RVfRdYUNTea1wMz\n1591V7buaT3GYOOIrW7d8mk9xnj9W2k9xiDtwTEux164A0jYEAO7YHc7v7y23OX9ymvF9zDveJ6D\n3Lq3L++w/LtEu9p7x6TI/huBfJa+mPk7D6hbf283CAGPkSckJKC0tNT5uKamBtHR0YiMjHQ57/jx\n4zh+/DgeeOAB5zFBENCpUyckJCSgtrbW5T3at/KJiNpTcuzW03h0x+5lqeepYc6gZ5xj4u0pMSeb\n4+DGEfAY+dChQ1FaWorKykoA1xLX0tLS3P+A1YqXXnoJZ86cAQC8/vrr6NOnD+Lj45GWloY9e/ag\ntbUVly9fxjvvvIN777030CIREUkmNYM+lAucMIOcpAi4RR4bG4ucnBxkZWWhpaUFvXr1wrJlywAA\nZWVlWLBgAQoLC9G7d28sWLAAM2bMgN1uR3x8PFatupaokZmZidOnTyMjIwMtLS2YOHEi7rzzTmVq\nRkTkhdQMejmZ9r5IyYZny5l8sQhiA9UaprfxF44Zmbf+rLv3ugdrSlewSb3ujmz4jvTc4jbzdx4I\n3Rg5V3YjoqDjntX62tSFtI2BnIiCjkHM2Nt9+rMmPskX8Bg5EVGgjBzEpAplNryaOg4ZKLUaHXnG\nFjkRBR1XFlM/Gz5UrWL2tgQfAzkRqcZTMOGe1epOLQtlDgJ7W4KPXetEpAopXaxqTOnSE7Wmlvna\n3UxNRh0y0DK2yIlIFb66WM2wZ3WourdD2Spmb0vwsUVORKowexdrKJO+QtkqZm9L8LFFTkSqMHtC\nWyiTvkLdKjZDb4uWMJATkSpCHUzU4E9Xuaeeh/JvP1O9m51rtJsLu9aJSBVG62L1t6vcU/e2lNcq\ngWu0mwdb5ESkGiN1sfrbVe6pR0LKa4n8wUBORCSBv8l77bu3/X1PIn8wkBMRSRBI8p6jR+Kn3fv5\n/VoiqRjIiUgVRts4Q07ynqfXHq8r19VnY7RrahRMdiMixeV/lm+4jTPkJO+1f+3xunK0CW0AgDah\nTTefDTdD0S6LIAhCqAvhD71tWq/mRvN6YOb6m7nuaQVDUHa+zO1439j+KJ74YQhKFDy+rvuw/LtE\ns9m1/tlIKbeZv/OAuvWPi+vm8Tl2rROR4spry0WPM7lLvyve6bXcZsBATkSK6xvXV/Q4k7v0u+Kd\nXsttBgzkRKS45//jedHjel7VTSl6XfFOr+U2Aya7EZHiJvWfhMuXvzfMqm5K0uuKd3ottxkw2U1l\nTP4wb/1Zd9bdbMxcd4DJbkRERBQABnIiIiIdYyAnIiLSMQZyIiIiHWMgJyIi0jFZ08+Ki4uxcuVK\nNDc3o0+fPsjOzkZUVJTLOfv27cO2bducj7/77jvU1NTg8OHDuOmmmzB48GD06NHD+fzUqVMxduxY\nOcUiIjK0vRUFWHNkpXMa2JxBz3AamIkFHMjr6uowb9487N69G0lJSVi+fDlWrFiBRYsWuZz3wAMP\n4IEHHgAAtLS0YPLkyZg2bRpuuukmnDx5EtHR0SgsLJRVCSIis+DmJdRRwF3rJSUlSE1NRVJSEgAg\nMzMTRUVF8DYtfdOmTejevTsmTZoEADh69CisViumTJmC9PR0rFu3Dna7PdAiEREZ3pojK0WPv/Lp\nqiCXhLTCZ4v88OHDmDFjhtvxmTNnIj4+3vk4Pj4eDQ0NaGxsdOteB6614Ldt24a33nrLecxut2PI\nkCF49tln0dTUhGnTpiEqKgqPPPJIgNUhIjI2bl5CHfkM5MOGDUN5uftORnl5eaLnW63ijfw33ngD\naWlpSExMdB57+OGHnf8fHh6ORx99FDt37vQayGNiImGzhfkqtqZ4W5HHDMxcf9bdnNSse9+4vuJb\nxMb11cRnroUyhFIo6h/wGHlCQgJKS0udj2tqahAdHY3IyEjR8w8cOIAFCxa4HNu3bx9SUlKQknJt\n9xxBEGCzeS9Sff2VQIscElyy0Lz1Z91ZdzXMuu0plzFyhycGzAn5Z27m6w7ocInWoUOHorS0FJWV\nlQCA/Px8pKWliZ576dIlnD59GrfffrvL8YqKCuTm5sJut6OpqQm7du3C/fffH2iRiIgMb1zyBGwc\nsRV9Y/vDZrWhb2x/bByxlYluJhZwizw2NhY5OTnIyspCS0sLevXqhWXLlgEAysrKsGDBAmc2+qlT\npxAXF4dOnTq5vMesWbOwePFipKeno7W1FaNGjcJDDz0kozpERMY3LnkCAzc5cfczlbGrybz1Z91Z\nd7Mxc90BHXatExERUegxkBMREekYAzkREZGOMZATERHpGAM5ERGRjjGQExER6RgDORERkY4xkBMR\nEemY7haEISIiouvYIiciItIxBnIiIiIdYyAnIiLSMQZyIiIiHWMgJyIi0jEGciIiIh2zhboARiQI\nAubNm4fk5GRMnTpV9Jzi4mKsXLkSzc3N6NOnD7KzsxEVFRXkkipPar2WLl2Kd999F9HR0QCAW265\nBWvWrAl2cWWTUl8zX2ujXGcxvv6dG/W6A77rbuTrXlhYiC1btsBisaBLly6YP38+UlNTXc4J+rUX\nSFFfffWVMGXKFGHAgAHC5s2bRc/59ttvhcGDBwv/+te/BEEQhJdffll48cUXg1dIlfhTr4cfflg4\ncr3fkAkAAAMJSURBVORI8AqnAin1Nfu1NsJ1FuPr37lRr7sgSPuNM+p1//rrr4UhQ4YINTU1giAI\nQnFxsTBs2DCXc0Jx7dm1rrBdu3Zh/PjxGD16tMdzSkpKkJqaiqSkJABAZmYmioqKIOh8bR6p9Wpu\nbkZ5eTm2bt2KsWPHYvbs2aiqqgpBieWRUl8zX2ujXGcxvv6dG/W6A77rbuTrHh4ejiVLluDmm28G\nAPTv3x8XLlxAc3Oz85xQXHsG8gAcPnwYffv2dftv3759WLhwIR544AGvr6+urkZ8fLzzcXx8PBoa\nGtDY2Kh20RXhqf6nT5+WVK+amhoMHjwYTz/9NAoLC3Hbbbdh5syZuvuRk3Id9X6tPZFSL6NcZzG+\n/p0b9boDvutu5Oves2dP3H333QCuDS/k5ORg+PDhCA8Pd54TimvPMfIADBs2DOXl5QG/vq2tTfS4\n1aqP+ypP9c/LyxM9v2O9EhMTsWnTJufjqVOnYv369Th79iwSExOVLayKpFxHvV9rT6TUyyjXORBG\nve5SmOG6X7lyBXPnzkV1dTU2b97s8lworr3xv1UalJCQgNraWufjmpoaREdHIzIyMoSlkk9qvY4f\nP459+/a5HBMEAZ06dQpKOZUipb5mvtZGuc6BMOp1l8Lo172qqgqTJk1CWFgYduzYgRtuuMHl+VBc\newbyEBg6dChKS0tRWVkJAMjPz0daWlpoC6UAqfWyWq146aWXcObMGQDA66+/jj59+rh0R+mBlPqa\n+Vob5ToHwqjXXQojX/eLFy9i8uTJuO+++7B69WpERES4nROSa69qKp2JPffccy4ZnceOHRPGjh3r\nfFxcXCykp6cLo0aNEqZNmybU19eHopiK81SvjvXft2+fMGbMGGHUqFHCI488InzzzTehKrIsYvU1\n87U26nX2pP2/c7NcdwdvdTfqdV+/fr2QkpIijB071uW/f/7znyG99tzGlIiISMfYtU5ERKRjDORE\nREQ6xkBORESkYwzkREREOsZATkREpGMM5ERERDrGQE5ERKRjDOREREQ69v8BzOf8nbnbTNoAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1229beef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty bad. But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features).\n",
    "\n",
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "\n",
    "・Define the graph within a logistic_regression() function that can be reused easily.\n",
    "\n",
    "・Save checkpoints using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "\n",
    "・Restore the last checkpoint upon startup if training was interrupted.\n",
    "\n",
    "・Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "\n",
    "・Add summaries to visualize the learning curves in TensorBoard.\n",
    "\n",
    "・Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve.\n",
    "\n",
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but we could also add them using sklearn.preprocessing.PolynomialFeatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -5.14696757e-02,   4.44198631e-01,\n",
       "          2.64912752e-03,   1.97312424e-01,  -1.36349734e-04,\n",
       "          8.76459084e-02],\n",
       "       [  1.00000000e+00,   1.03201691e+00,  -4.19741157e-01,\n",
       "          1.06505890e+00,   1.76182639e-01,   1.09915879e+00,\n",
       "         -7.39511049e-02],\n",
       "       [  1.00000000e+00,   8.67891864e-01,  -2.54827114e-01,\n",
       "          7.53236288e-01,   6.49368582e-02,   6.53727646e-01,\n",
       "         -1.65476722e-02],\n",
       "       [  1.00000000e+00,   2.88850997e-01,  -4.48668621e-01,\n",
       "          8.34348982e-02,   2.01303531e-01,   2.41002535e-02,\n",
       "         -9.03185778e-02],\n",
       "       [  1.00000000e+00,  -8.33439108e-01,   5.35056649e-01,\n",
       "          6.94620746e-01,   2.86285618e-01,  -5.78924095e-01,\n",
       "          1.53179024e-01]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the logistic_regression() function to create the graph. We will leave out the definition of the inputs X and the targets y. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the logistic_regression() function. We will also create the FileWriter to save the summaries to the log directory for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file.\n",
    "\n",
    "We can try interrupting training to verify that it does indeed restore the last checkpoint when you start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.629985\n",
      "Epoch: 500 \tLoss: 0.161224\n",
      "Epoch: 1000 \tLoss: 0.119032\n",
      "Epoch: 1500 \tLoss: 0.0973292\n",
      "Epoch: 2000 \tLoss: 0.0836979\n",
      "Epoch: 2500 \tLoss: 0.0743758\n",
      "Epoch: 3000 \tLoss: 0.0675021\n",
      "Epoch: 3500 \tLoss: 0.0622069\n",
      "Epoch: 4000 \tLoss: 0.0580268\n",
      "Epoch: 4500 \tLoss: 0.054563\n",
      "Epoch: 5000 \tLoss: 0.0517083\n",
      "Epoch: 5500 \tLoss: 0.0492377\n",
      "Epoch: 6000 \tLoss: 0.0471673\n",
      "Epoch: 6500 \tLoss: 0.0453766\n",
      "Epoch: 7000 \tLoss: 0.0438187\n",
      "Epoch: 7500 \tLoss: 0.0423742\n",
      "Epoch: 8000 \tLoss: 0.0410892\n",
      "Epoch: 8500 \tLoss: 0.0399709\n",
      "Epoch: 9000 \tLoss: 0.0389202\n",
      "Epoch: 9500 \tLoss: 0.0380107\n",
      "Epoch: 10000 \tLoss: 0.0371557\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97979797979797978"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97979797979797978"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFKCAYAAADmCN3IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBRcEtwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122a788d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "$ tensorboard --logdir=tf_logs\n",
    "\n",
    "Now we can play around with the hyperparameters (e.g. the batch_size or the learning_rate) and run training again and again, comparing the learning curves. We can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  logdir: tf_logs/logreg-run-20170905164241/\n",
      "  batch size: 54\n",
      "  learning_rate: 0.00443037524522\n",
      "  training: .....................\n",
      "  precision: 0.979797979798\n",
      "  recall: 0.979797979798\n",
      "Iteration 1\n",
      "  logdir: tf_logs/logreg-run-20170905164618/\n",
      "  batch size: 22\n",
      "  learning_rate: 0.00178264971514\n",
      "  training: .....................\n",
      "  precision: 0.979797979798\n",
      "  recall: 0.979797979798\n",
      "Iteration 2\n",
      "  logdir: tf_logs/logreg-run-20170905165506/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.00203228544324\n",
      "  training: .....................\n",
      "  precision: 0.969696969697\n",
      "  recall: 0.969696969697\n",
      "Iteration 3\n",
      "  logdir: tf_logs/logreg-run-20170905165748/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.00449152382514\n",
      "  training: .....................\n",
      "  precision: 0.979797979798\n",
      "  recall: 0.979797979798\n",
      "Iteration 4\n",
      "  logdir: tf_logs/logreg-run-20170905170119/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.0796323472178\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 5\n",
      "  logdir: tf_logs/logreg-run-20170905170503/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.000463425058329\n",
      "  training: .....................\n",
      "  precision: 0.912621359223\n",
      "  recall: 0.949494949495\n",
      "Iteration 6\n",
      "  logdir: tf_logs/logreg-run-20170905170714/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.0477068184194\n",
      "  training: .....................\n",
      "  precision: 0.98\n",
      "  recall: 0.989898989899\n",
      "Iteration 7\n",
      "  logdir: tf_logs/logreg-run-20170905170955/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.000169404470952\n",
      "  training: .....................\n",
      "  precision: 0.9\n",
      "  recall: 0.909090909091\n",
      "Iteration 8\n",
      "  logdir: tf_logs/logreg-run-20170905171319/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.0417146119941\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 9\n",
      "  logdir: tf_logs/logreg-run-20170905171644/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.000107429229684\n",
      "  training: .....................\n",
      "  precision: 0.882352941176\n",
      "  recall: 0.757575757576\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "        X, y, learning_rate=learning_rate)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "    final_model_path = \"./my_logreg_model_%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "\n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "documents",
   "language": "python",
   "name": "documents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
