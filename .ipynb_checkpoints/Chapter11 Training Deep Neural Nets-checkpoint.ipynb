{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"Chapter11\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses tensorflow.contrib.layers.fully_connected() rather than tf.layers.dense() (which did not exist when this chapter was written). It is now preferable to use tf.layers.dense(), because anything in the contrib module may change or be deleted without notice. The dense() function is almost identical to the fully_connected() function. The main differences relevant to this chapter are:\n",
    "\n",
    "・several parameters are renamed: scope becomes name, activation_fn becomes activation (and similarly the _fn suffix is removed from other parameters such as normalizer_fn), weights_initializer becomes kernel_initializer, etc.\n",
    "\n",
    "・the default activation is now None rather than tf.nn.relu.\n",
    "\n",
    "・it does not support tensorflow.contrib.framework.arg_scope().\n",
    "\n",
    "・it does not support regularizer params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126dbff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Leaky ReLU in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network on MNIST using the Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.951\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9666\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9722\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9768\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9778\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a63f3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function outperforms the other activation functions very significantly for deep neural networks, so we should really try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd0VHX+//HXnZZMqBGIVA2oICtB0WXFXRXFn1+Usmps\noBJA6VIsoCgIqEiRpkZUWgi4CMIuirCrlHWlqCgguijrimFp0kIRSJmZzMz9/TESwYSacqc8H+fM\nyZQ7977Hy515ee/73o9hmqYpAACACGazugAAAICSItAAAICIR6ABAAARj0ADAAAiHoEGAABEPAIN\nAACIeAQaoIz5/X698cYbuvXWW9WkSRPdcMMNeu6553Tw4MHCadLT09WoUaNib3369JEkLVy4UDfe\neOMpl5Oenq6OHTsW+9qNN96ohQsXlu4Hk7Rz50598sknkqRdu3apUaNG2r59e4nnu3btWv3www+S\nzvy5S+LQoUPq0KGDUlJSNGnSpDJZxnEfffSRsrOzJZ1+XQE4Pw6rCwCi3YQJE7Rq1SqNGDFCycnJ\n2rNnj8aNG6fu3bvrb3/7mwzDkCQ1bdpUb7zxRpH3x8XFlXfJZ+3ZZ5/V1VdfrZtuukm1atXSmjVr\ndMEFF5R4vp07d9bMmTPVsGFDtWnTRjfddFPJiy3GBx98oJ07d+r9999XtWrVymQZkvTTTz9pwIAB\nWrZsmSTp4YcfVqdOncpseUAsItAAZWzhwoV6/vnn9ac//UmSVKdOHY0fP17/7//9P33zzTe66qqr\nJEkOh0M1atSwstQSsdvtZVJ/fHy84uPjS32+kpSTk6N69erpkksuKZP5H/fb65dWqFChTJcHxCIO\nOQHlYO3atQoEAoWP69Wrp3/84x+6/PLLLawqJCcnR0OGDNF1112nJk2aqHXr1lq6dGnh64cPH9aT\nTz6pa665Rtddd51eeukl+f1+DR48WF9++aXeeustderU6aRDTuPHj1eHDh1OWs7UqVOVmpoqScrK\nylK3bt3UrFkzpaSkqGPHjtqyZYskqVWrVpKkrl27Kj09vcghp6ysLD3yyCO6+uqrdf311ys9PV3B\nYFBS6FDO448/rhdeeEHXXHONWrRooSlTphT7uQcPHqz09HRt3LhRjRo10q5du9SqVSstWLCgcJov\nvvhCjRo1kt/vL/x8S5cu1a233qqUlBR1795dhw4dKpz+s88+U2pqqq688kq1bdtWH3/8sSTplltu\nkST93//9nxYuXFjkkNPGjRvVsWNHXXXVVWrVqpXmzJlzUp0jR47UE088oauuuqrMDh8CkY5AA5Sx\ntLQ0zZ07VzfffLOGDh2qv//97zp69KguueSSMtvzcC5Gjx6trKwsZWRkaMmSJWrevLmee+45+Xw+\nSVLfvn31008/adasWZo8ebJWrFihadOmaciQIWrWrJk6d+6s9PT0k+bZrl07ff3119q/f3/hcx99\n9JHatm0r0zTVp08f1a5dW4sWLdK8efMUDAb18ssvS5L++te/SpJeeeUVPfzwwyfN99ChQ3rggQeU\nlJSkBQsWaMSIEZozZ44yMjIKp1m+fLnsdrsWLlyobt26aeLEifrxxx+LfO4hQ4bo4YcfVtOmTbVm\nzRrVqlXrrP57TZkyRePHj9df/vIXfffdd5oxY4akUNDq0aOHWrVqpUWLFum+++7TgAEDtHPnzsKQ\n9O6776pNmzYnzS8rK0udO3dW8+bN9d5776lfv34aN26cPvzww8Jp5s2bp8aNG2vx4sVq3bq1RowY\noZ9//vms6gViBYecgDL26KOPqn79+nrnnXe0cOFCLViwQHFxcerfv7+6detWON3XX3+tZs2aFXn/\nxIkTdfPNN5dZfddcc43S0tLUqFEjSaH+jgULFmjfvn3yeDxav369li1bposvvliSNGLECGVnZ6tS\npUpyOp1yu92qWrWqcnJyCud5+eWXq0GDBlq+fLkefPBB7dy5U5s3b9bkyZOVn5+ve++9Vx07diw8\n9HLXXXcV7kk53oNTpUqVIodmlixZovj4eL3wwgtyOp265JJLlJ2drVdffbXwv2WlSpU0ePBg2e12\ndevWTdOmTdO3336rSy+99KR5VapUSQkJCed8qK9v37668sorJUnt27fXpk2bJIWCWEpKivr27StJ\nSk5OVm5urnJzcws/U2JiYpEQO3/+fDVq1EhPPPGEJKl+/frKysrS9OnTdfvtt0uSGjZsqO7du0uS\nBgwYoNmzZ2vLli1q3rz5WdcNRDsCDVAO2rRpozZt2ujo0aP67LPP9O6772rcuHGqX79+4eGIxo0b\nF3umzdn+2DocjiK9GscFg0E5HMVv7nfeeadWrFihBQsWaOvWrfruu+8K3/Pjjz+qYsWKhWFGklq2\nbHlW9bRp06Yw0Hz00Udq1qxZ4V6Qjh07atGiRfr222+1detWbd68WVWrVj3jPLOystS4cWM5nc7C\n55o1a6bDhw8XHvqpU6eO7HZ74esVKlRQQUHBWdV8Ni666KLC+xUrVpTf7y+s7Yorrjhp2uNnqO3a\nteuU88vKyioMSMc1a9bspMNO9erVO2mZkgqXCyCEQ05AGfr+++81cuTIwseVK1fWbbfdpoyMDDVp\n0kSffvpp4WtxcXG6+OKLi9wSEhLOalmVK1fWsWPHijxvmqaOHTumypUrF/u+p556SmPGjFGlSpXU\nsWPHk3pOTgwO56pt27Zat26dDh8+XHi4SZJyc3N1zz336IMPPlCDBg3Uv39/PfXUU2c1z+IO0R3v\nnzn+tyQ1/9aJfU/H/Xb+x0Pk+S73VJ/pxGUXN+9ThVcgVhFogDIUCAT09ttv6+uvvz7pecMwVKlS\npVI5xfm44w25hw8fPun577//Xh6PR40bNy7ynpycHC1ZskQTJkzQgAEDdOutt+rIkSOSQj+YycnJ\nysnJ0c6dOwvfs2DBAqWlpZ2xnvr166tRo0ZasGCBvv/+e912222SpC+//FJ79+7V22+/rW7duumP\nf/yjdu/efVY/0A0aNNDmzZtP2uOyceNGVa1atVT+WzqdTuXm5hY+PvFzn8nFF1+s//znPyc917Vr\nV82fP7/w1PziNGjQQN98881Jz23cuFH169c/62UDINAAZeqKK67QzTffrL59++q9997Tzp07tWnT\nJk2aNEn/+c9/dM899xRO6/f7lZ2dXeR24gX4fD6fVq1addJtw4YNkqSrr75ajRo1Uv/+/fXVV19p\n586dWrlypQYNGqT27dvrwgsvLFKfy+WS2+3WsmXLtGvXLq1Zs0YvvPBC4bIuvfRS/fGPf9Szzz6r\n77//XuvXr9ebb76p66+/XlLocM6OHTtOqvFEbdq00ZtvvqnmzZurevXqkqSqVasqPz9fy5cv165d\nu7RgwQLNmTOnsAlZkhISErRly5Yie5zatWunYDCoYcOGKSsrS//85z+Vnp6uDh06yGYr+ddZSkqK\nFi5cqB9++EFffvmlZs6cedbv7dixo7755htNnTpV27dv16xZs7Rx40Zdd911hXvZvv/++5MCkyQ9\n8MAD+uGHHzRx4kT973//0/vvv6933nlHDz30UIk/DxBL6KEBytgrr7yiqVOnasqUKRo+fLhcLpea\nN2+uOXPmqGbNmoXT/fvf/y4MCieqWrWqvvjiC0mhU6iPN4ced9lll2nJkiWy2WyaPn164d6Ww4cP\nKykpSe3atStsVP0tl8ulcePGaezYsZozZ47q1q2rXr16KT09XZs3b1bDhg318ssv64UXXlCHDh1U\noUIFpaamFjbg3n///Xr66afVrVu3Imc6SaHDTuPHjy883CSF+kP69u2rF198UV6vVw0bNtTw4cP1\nzDPPaPfu3apdu7a6dOmiCRMm6Keffjrp1PYKFSpo+vTpGjlypO68805dcMEFSktLU69evc5hjZza\nY489pmeeeUapqamqX7++HnvsMQ0YMOCs3luvXj1NnjxZ48ePV3p6uho0aKDJkycX9r+kpqbqySef\n1MCBA096X82aNTVlyhS9/PLLysjIUO3atTV48GDde++9pfKZgFhhmByIBQAAEY5DTgAAIOIRaAAA\nQMQj0AAAgIhHoAEAABGPQAMAACJeuZ+2nZ1d9EqmkS4xMUGHD+dZXQbOgPUU/q65polsNkPr1m2y\nuhScRjRuS//5j01t2iQoN9fQsGEe9e1besNlWCUa11ONGpVO+Rp7aEqBw2E/80SwHOsJKB3Rti0d\nPiylpbmVm2soNbVAjz4a+WFGir71dCYEGgBAzPL7pR493Nq+3aaUlIAmTvToNCNVIIwRaAAAMevF\nF+O0cqVD1asHNWtWvs5yLFiEIQINACAmzZ/v0JtvuuRwmMrI8KhuXS6cH8kINACAmPP11zY9+WS8\nJOmll7xq0SJgcUUoKQINACCm7N9vqEsXt7xeQ506+dSlS3Q0Acc6Ag0AIGb4fNLDD8dr926b/vAH\nv0aP9tIEHCUINACAmPHMM3H68kuHatUKasYMj1wuqytCaSnxhfX+8pe/aO7cuTIMQ/Xq1dPIkSNV\nrVq10qgNAIBSk5np1NtvuxQXZ2rWrHxdeCFNwNGkRHtovv32W2VkZGjevHlasmSJkpOT9eqrr5ZW\nbQAAlIq1a+169tk4SdKECR5ddVXQ4opQ2koUaJo0aaKlS5eqUqVK8nq92rdvn6pWrVpatQEAUGK7\ndhl6+OF4+f2GevXy6b77/FaXhDJgmKZZ4n1uK1as0JAhQ+RyufT2228rOTn5lNP6/YGYuxwzgLNz\n/Ltj27ZtltaB6JGXJ91wg/TVV9Ktt0r/+IfkKPdRDFEeSiXQHDd//nxNmTJFy5cvl81W/M6faByc\nskaNSlH5uaIN6yn8MThlZIiUbck0pd6947VwoVMXXxzUsmW5Sky0uqryEynr6VyU2eCU27dv1/r1\n6wsf33333dq9e7eOHDlSktkCAFBib7zh1MKFTiUkmJo9Oz+mwkwsKlGgyc7O1hNPPKFDhw5JkhYv\nXqzLLrtMifyrAQBY6OOP7XrxxVAT8OTJHjVuTBNwtCvRkcTf//736tWrl9LS0mS325WUlKTJkyeX\nVm0AAJyzrVsN9ezpVjBo6MknvWrblibgWFDi1qgHHnhADzzwQGnUAgBAieTkSJ07u3XkiKHbbivQ\noEE+q0tCOeFKwQCAqBAMSn36xOu//7WrUaOAJk/26BTnpyAKsaoBAFFh3DiXPvrIqSpVQlcCrnTq\nE2IQhQg0AICIt2SJQxMmxMlmMzVlSr4aNGBYg1hDoAEARLTNm23q2zdekjR0qFetWgUsrghWINAA\nACLW4cOhJuC8PEOpqQV69NECq0uCRQg0AICI5PdL3bu7tX27TU2bBjRpkkeGYXVVsAqBBgAQkV54\nIU6rVjlUvXpQmZn5crutrghWItAAACLO/PkOvfWWSw6HqYwMj+rWpQk41hFoAAARZeNGm558MtQE\nPGqUVy1a0AQMAg0AIILs22eoSxe3vF5DnTr51KULTcAIIdAAACKCzyc98ki89uyx6Q9/8Gv0aK/V\nJSGMEGgAAGHPNKVnnonTl186VLt2UBkZHrlcVleFcEKgAQCEvcxMp95+26X4eFOZmflKSqIJGCcj\n0AAAwtrnn9s1ZEicJGnCBI+uuipocUUIRwQaAEDY2rXL0COPxMvvN9Srl0/33uu3uiSEKQINACAs\n5eWFhjU4cMCmli39GjaMJmCcGoEGABB2TFN64ol4bdpk18UXBzV1ar4cDqurQjgj0AAAws7kyU4t\nXOhUQoKp2bPzlZhodUUIdwQaAEBY+fhju0aODDUBT57sUePGNAHjzAg0AICwsXWroR493AoGDQ0c\n6FXbtjQB4+wQaAAAYeHYMSktza2jRw3dfnuBBg70WV0SIgiBBgBguWBQevTReP3wg12NGgU0ebJH\nNn6hcA745wIAsNy4cS599JFTVaqYmjUrXxUrWl0RIg2BBgBgqSVLHJowIU42m6mpU/PVoAHDGuDc\nEWgAAJbZvNmmvn3jJUnPPefVzTcHLK4IkYpAAwCwxKFDoSbgvDxDd99doD59CqwuCRGMQAMAKHd+\nv9S9u1s7dtjUtGlAEyd6ZBhWV4VIRqABAJS755+P0+rVDlWvHtSsWflyu62uCJGOQAMAKFfvvuvQ\nlCkuORymMjI8qlOHJmCUHIEGAFBuNm60aeDAUBPw6NFetWhBEzBKB4EGAFAu9u0z1KWLW16vobQ0\nnzp3pgkYpYdAAwAoc16v9PDDbu3ZY9O11/o1apTX6pIQZQg0AIAyZZrSs8/Gad06u2rXDmrGDI9c\nLqurQrQh0AAAylRmplNvv+1SfLypzMx8JSXRBIzSR6ABAJSZzz+3a8iQOEnSxIkeXXVV0OKKEK0I\nNACAMrFrl6FHHomX32+od2+f7rnHb3VJiGIEGgBAqcvLkzp3duvAAZtatvTruedoAkbZItAAAEqV\naUqPPx6vTZvsSk4OaurUfDkcVleFaEegAQCUqtdfd+m995yqUMHU7Nn5Sky0uiLEAgINAKDUfPyx\nXSNHhs7JnjzZo8svpwkY5YNAAwAoFVlZhnr0cMs0DQ0a5FWbNjQBo/wQaAAAJXbsWKgJ+OhRQ7ff\nXqAnn/RZXRJiDIEGAFAiwaDUp49bP/xg1+WXBzR5skc2fl1QzvgnBwAokZdfdmnpUoeqVAldCbhi\nRasrQiwi0AAAztvixQ5NnBgnm83U1Kn5atCAYQ1gDQINAOC8bN5sU79+8ZKkYcO8uvnmgMUVIZYR\naAAA5+zQISktza28PEP33FOg3r0LrC4JMY5AAwA4J36/1L27Wzt22HTllQFNmOCRYVhdFWIdgQYA\ncE4GDZJWr3aoevWgMjPz5XZbXRFAoAEAnIN333XolVckp9NURoZHderQBIzwQKABAJyVr76yaeDA\nUBPwqFFetWhBEzDCB4EGAHBG+/YZ6tLFLa/XUK9eUufONAEjvBBoAACn5fVKXbu6tXevTdde69er\nr1pdEVCUo6QzWLRokWbMmCHDMOR2uzVkyBClpKSURm0AAIuZpvTMM3Fav96u2rWDmjHDI5eLSwEj\n/JQo0GzdulXjxo3TwoULlZSUpJUrV6pfv3765JNPSqk8AICVZs506i9/cSk+3tSsWflKSqIJGOGp\nRIecXC6XRo4cqaSkJElSkyZNdODAAfl8jLIKAJHus8/sGjo0TpI0caJHV14ZtLgi4NQM0zRLJW6b\npqlBgwbJ5/PptddeO+V0fn9ADoe9NBYJIMokJydLkrZt22ZpHZC2b5d+/3vpwAFp4EBp3DirKwJO\nr8Q9NJKUl5enwYMHa+/evZo+ffpppz18OK80FhlWatSopOzsY1aXgTNgPYW/YNCUzWawniyWlye1\nb5+gAwfsuukmv558Ml/Z2b++zrYUGaJxPdWoUemUr5X4LKfdu3erQ4cOstvtmj17tipXrlzSWQIA\nLGKa0uOPx2vTJruSk4OaOjVfdnaqIwKUaA/Nzz//rIceekipqanq27dvadUEALBIerpL773nVIUK\npmbPzlfVqlZXBJydEgWauXPnas+ePVq+fLmWL19e+HxmZqYSExNLXBwAoPz88592vfSSS5L0xhse\nXX45TcCIHCUKNL1791bv3r1LqxYAgEWysgz17OmWaRoaNMir22/3W10ScE64UjAAxLhjx6S0NLeO\nHjXUpk2BnnySS28g8hBoACCGBYNSnz5ubdli1+WXB/T66x7Z+GVABOKfLQDEsJdfdmnpUoeqVg1d\nCbgioxogQhFoACBGLV7s0MSJcbLZTE2dmq/69RnWAJGLQAMAMei772zq1y9ekjR8uFc33RSwuCKg\nZAg0ABBjDh2SOnd2Ky/P0D33FKhXrwKrSwJKjEADADHE75e6d3drxw6brroqoAkTPDIMq6sCSo5A\nAwAx5Pnn47R6tUPVqwc1c2a+3G6rKwJKB4EGAGLEvHkOTZniktNpKiPDozp1aAJG9CDQAEAM+Oor\nmwYNCjUBjx7tVYsWNAEjuhBoACDK7dtnqEsXt7xeQ507+5SWRhMwog+BBgCimNcrde3q1t69NrVo\n4ddLL3mtLgkoEwQaAIhSpikNHhyn9evtqlMnqBkzPHK5rK4KKBsEGgCIUhkZTs2Z41J8vKnMzHzV\nqEETMKIXgQYAotBnn9n13HNxkqRJkzy68sqgxRUBZYtAAwBRZudOQ488Ei+/39Cjj/p0991+q0sC\nyhyBBgCiSF5eaFiDgwdtuvlmv4YOpQkYsYFAAwBRwjSlxx6L17ff2lW/flBTpuTLbre6KqB8EGgA\nIEqkp7v0/vtOVahgatasfFWtanVFQPkh0ABAFFixwq6XXgqdk/3GGx5dfjlNwIgtBBoAiHBZWYZ6\n9XLLNA099ZRXt99OEzBiD4EGACLYsWNSWppbR48aatOmQE884bO6JMASBBoAiFDBoNS7t1tbttjV\nuHFAr7/ukY1vdcQo/ukDQIQaO9alZcscqlo1dCXgihWtrgiwDoEGACLQ4sUOTZoUJ5vN1LRp+apf\nn2ENENsINAAQYb77zqZ+/eIlScOHe9WyZcDiigDrEWgAIIIcPGioc2e38vIM3XtvgXr1KrC6JCAs\nEGgAIEL4/VKPHvHascOmq64KaPx4jwzD6qqA8ECgAYAIMWJEnFavdqhGjaAyM/PldltdERA+CDQA\nEAHmzXNo6lSXnE5TGRke1a5NEzBwIgINAIS5DRtsGjgw1AQ8ZoxX115LEzDwWwQaAAhj+/YZ6trV\nLZ/PUJcuPnXqRBMwUBwCDQCEKa9X6tLFrb17bWrRwq+RI71WlwSELQINAIQh05QGD47Thg121akT\n1IwZHrlcVlcFhC8CDQCEoYwMp+bMccntNjVrVr5q1KAJGDgdAg0AhJlPP7Vr6NA4SdKkSR41bRq0\nuCIg/BFoACCM7NxpqFu3eAUChh591KfUVL/VJQERgUADAGEiN1fq3NmtgwdtatXKr6FDaQIGzhaB\nBgDCgGlKjz8er2+/tat+/aDeeitfdrvVVQGRg0ADAGEgPd2l9993qkIFU7Nn56tqVasrAiILgQYA\nLLZihV0vvRQ6J/uNNzxq1IgmYOBcEWgAwEI//mioZ0+3TNPQ0097dfvtNAED54NAAwAWOXpUSktz\n69gxQ23bFujxx31WlwRELAINAFggGJT69HHrxx/tatw4oPR0j2x8IwPnjc0HACwwdqxLy5Y5lJgY\nuhJwxYpWVwRENgINAJSzDz5waNKkONlspqZOzVdyMsMaACVFoAGAcvTttzb17x8vSRoxwquWLQMW\nVwREBwINAJSTgwcNdeniVl6eoXvvLVDPngVWlwREDQINAJSDggKpe/d47dhhU7NmAY0f75FhWF0V\nED0INABQDkaMiNOaNQ7VqBHUzJn5crutrgiILgQaAChjc+c6NG2aS06nqZkz81W7Nk3AQGkj0ABA\nGdqwwaZBg0JNwGPGePWHPzCsAVAWCDQAUEb27g01Aft8hrp29alTJ5qAgbJS4kBjmqYGDx6sGTNm\nlEY9ABAVvF6pa1e39u2z6brr/Bo50mt1SUBUK1GgycrKUufOnfXhhx+WVj0AEPFMU3r66Tht2GBX\nnTpBTZ/ukdNpdVVAdHOU5M1z5sxRamqqateuXVr1AEDEy8hw6p13XHK7Q8Ma1KhBEzBQ1koUaIYN\nGyZJWrt2bakUAwCRbs0au4YOjZMkTZrkUdOmNAED5aFEgeZ8JCYmyOGwl/diy1yNGpWsLgFngfUU\n3my20JXmInU9bdsmde8uBQLSU09JPXtG78VmInUdxZpYWk/lHmgOH84r70WWuRo1Kik7+5jVZeAM\nWE/hLxg0ZbMZEbmecnOldu0SdPCgXa1a+fX44/nKzra6qrLBthQZonE9nS6gcdo2AJSQaUqPPRav\n776zq0GDoN56K1/26NsRDYQ1Ag0AlNBrr7m0aJFTFSuamj07X1WrWl0REHtK5ZDTmDFjSmM2ABBx\nli+3a9QolyTpjTfy1bAhTcCAFdhDAwDn6ccfDfXq5ZZpGnr6aa9uuy1gdUlAzCLQAMB5OHpUSktz\n69gxQ+3aFejxx31WlwTENAINAJyjQEDq3dutH3+0q3HjgF57zSMb36aApdgEAeAcjR3r0vLlDiUm\nhq4EXLGi1RUBINAAwDlYtMihV16Jk81maurUfCUnM6wBEA4INABwlr791qYBA+IlSc8/71XLljQB\nA+GCQAMAZ+HgQUOdO7uVl2fo/vsL1KNHgdUlATgBgQYAzqCgQOrePV47d9rUrFlA48Z5ZBhWVwXg\nRAQaADiDESPitGaNQ0lJQWVm5is+3uqKAPwWgQYATmPuXIemTXPJ6TSVkZGvWrVoAgbCEYEGAE5h\n/XqbBg0K7Y4ZO9arP/yBYQ2AcEWgAYBi7N1rqGtXt3w+Q127+vTQQzQBA+GMQAMAv+HxSF27urVv\nn03XXefXyJFeq0sCcAYEGgA4gWlKTz8drw0b7KpbN6gZMzxyOq2uCsCZEGgA4AQzZjg1d65Tbndo\nWIPq1WkCBiIBgQYAfrFmjV3PPRcnSZo0yaOUFJqAgUhBoAEASdu3G+rWLV6BgKF+/bxKTfVbXRKA\nc0CgARDzcnOlzp3dOnTIpltu8evZZ31WlwTgHBFoAMQ005QGDIjX5s12NWgQ1Ftv5ctut7oqAOeK\nQAMgpr32mksffOBUxYqmZs/OV5UqVlcE4HwQaADErOXL7Ro1yiXDMPXmm/lq2JAmYCBSEWgAxKQt\nW2zq1cst0zT09NM+tW4dsLokACVAoAEQc44ckdLS3Dp2zFC7dgV6/HGagIFIR6ABEFMCAal3b7ey\nsmxq3Dig117zyDCsrgpASRFoAMSUMWNcWrHCocTEUBNwxYpWVwSgNBBoAMSMRYscevXVONntpqZN\ny9fFFzOsARAtCDQAYsKmTTYNGBAvSRoxwqsbb6QJGIgmBBoAUe/AAUNduriVl2fo/vsL1KNHgdUl\nAShlBBoAUa2gQOrePV47d9p09dUBjRtHEzAQjQg0AKLa8OFx+vRTh5KSgpo5M1/x8VZXBKAsEGgA\nRK133nFo+nSXXC5TM2fmq1YtmoCBaEWgARCV1q+36amnQrtjxo71qnlzhjUAohmBBkDU2bvXUNeu\nbvl8hh6ytE7wAAAaAklEQVR+2KcHH6QJGIh2BBoAUcXjkbp2dWvfPpv++Ee/XnzRa3VJAMoBgQZA\n1DBN6amn4rVhg1116wY1fbpHTqfVVQEoDwQaAFFj+nSn5s1zyu02NWtWvqpXpwkYiBUEGgBRYfVq\nu4YNi5MkvfqqRykpNAEDsYRAAyDibd9uqHv3eAUChvr18+rOO/1WlwSgnBFoAES03Fypc2e3Dh2y\n6ZZb/Hr2WZ/VJQGwAIEGQMQyTWnAgHht3mzXJZcE9dZb+bLbra4KgBUINAAi1quvuvTBB05VrBhq\nAq5SxeqKAFiFQAMgIi1bZtfo0S4Zhqk338xXw4Y0AQOxjEADIOJs2WJTr15umaahwYN9at06YHVJ\nACxGoAEQUY4ckdLS3MrJMdS+fYEee4wmYAAEGgARJBCQevd2KyvLpsaNA3r1VY8Mw+qqAIQDAg2A\niDF6tEsrVjiUmGhq9ux8VaxodUUAwgWBBkBEeP99h157LU52u6np0/N18cUMawDgVwQaAGFv0yab\nBgyIlyQ9/7xXN9xAEzCAkxFoAIS1AwcMdeniVn6+ofvvL1D37gVWlwQgDBFoAIStggKpW7d47dxp\n09VXBzRuHE3AAIpHoAEQtoYNi9NnnzmUlBRUZma+4uOtrghAuCLQAAhL77zj0IwZLrlcpmbOzFfN\nmjQBAzg1Ag2AsLNunU1PPRXaHfPyyx41b86wBgBOr8SB5pNPPlH79u3VunVr9e/fXzk5OaVRF4AY\nFQhIXbu65fMZeuQRnx54wG91SQAiQIkCzaFDh/TMM88oPT1dS5cuVb169TR+/PjSqg1AjDFNaf9+\naf9+m/70J79eeMFrdUkAIkSJAs2aNWuUkpKi5ORkSVLHjh21ePFimSbHugGcu2PHJJ9Pqls3qGnT\nPHI6ra4IQKRwlOTNe/fuVc2aNQsf16xZUzk5OcrNzVXFU1yT/JprmpRkkWHJZjMUDBLiwh3rKbz5\n/dKRIz9Jkjye+mrd2uKCcEpsS5EhGtfTjh3bT/laiQJNMFh8o57NduodPzZbdF5EIlo/V7RhPYWv\nI0dCfw1DqlCB9RTu2JYiQyytpxIFmlq1aumbb74pfLxv3z5VqVJFCQkJp3zPunWbSrLIsFSjRiVl\nZx+zugycAespfK1YYdcDDyTIMJJVp050fk9EE7alyBBr66lEPTTXX3+9vvnmG23btk2SNG/ePN1y\nyy2lUReAGOH3hy6gJ0mVK5uy2y0uCEBEKtEemmrVqmn06NHq37+/CgoKdNFFF2ns2LGlVRuAGLBw\noUM//mjXRRcFdYqj2ABwRiUKNJLUsmVLtWzZsjRqARBj/H5p/PjQ3pmBA70aN87iggBELK4UDMAy\n8+c7tG2bTQ0aBHXPPVxAD8D5I9AAsITPJ02c+OveGUeJ9xcDiGUEGgCWmDvXqR07bLrssoDuuou9\nMwBKhkADoNzl5Unjx7skSU895ePMJgAlRqABUO6mTXNp3z6brrwyoPbt2TsDoOQINADK1eHDUnp6\naO/M0KFenebC4gBw1vgqAVCuXn01TkePGrrxRr9atgxYXQ6AKEGgAVBudu0yNGNGaAjt557zWlwN\ngGhCoAFQbkaOjJPXa+iuuwp05ZVcFhhA6SHQACgXGzbYtHChU3FxpoYOZe8MgNJFoAFQ5kxTGjYs\nXpLUq5dP9eqZFlcEINoQaACUucWLHVq3zq7q1YPq399ndTkAohCBBkCZys+Xnn8+NMTB4ME+Vapk\ncUEAohKBBkCZSk93aedOm664IqAHHiiwuhwAUYpAA6DMbN9u6PXXQxfRGz2aASgBlB0CDYAyM2xY\nnDweQ3ffXaAWLbiIHoCyQ6ABUCY+/tiuDz90qkIFU8OHc5o2gLJFoAFQ6vLzpaefDp2m/eSTXtWs\nyWnaAMoWgQZAqZs0yaXt221q3Dignj1pBAZQ9gg0AErVf/9r0+TJoUbg8eM9cjotLghATCDQACg1\nwaA0aFCcCgoMpaX51Lw54zUBKB8EGgClZvZsp9audah69SDjNQEoVwQaAKVi1y6j8IrAY8d6VbWq\nxQUBiCkEGgAlZprSwIHxys011LZtgdq391tdEoAYQ6ABUGLvvuvQxx87VLWqqTFjONQEoPwRaACU\nyE8/GRo6NHTNmRdf9OjCC7nmDIDyR6ABcN6CQal//3gdPWrottsKdN99HGoCYA0CDYDzNnOmU6tX\nO1StWlDjx3tlGFZXBCBWEWgAnJcffzT0wguhs5rGj/cqKYlDTQCsQ6ABcM7y86Vu3dzKzzd0330F\natuWQ00ArEWgAXDOhg+P0+bNdtWvH9SYMR6rywEAAg2Ac/PBBw5lZrrkcpmaPj1fFStaXREAEGgA\nnINt2ww9/njoFO3nn/cqJYWxmgCEBwINgLPi80k9e7p17JihNm0K9PDDBVaXBACFCDQAzsrIkXHa\nuNGuevWCeuUVD6doAwgrBBoAZ7R0qV1vveWSw2FqypR8Bp4EEHYINABOa8sWm/r0cUuSnn3Wq9//\nnr4ZAOGHQAPglH7+WUpLC/XNtGtXoD596JsBEJ4INACK5feHmoCzsmz63e8Ceu01j2x8YwAIU3w9\nASjWCy/E6V//Co3TNHs215sBEN4INACKmDfPUdgEnJHh0UUXMU4TgPBGoAFwkvXrbRo4MHTxvDFj\nvLruuoDFFQHAmRFoABTats1Q585u+XyGunb1KS2NJmAAkYFAA0CSlJ1t6P77E5SdbdMNN/g1cqTX\n6pIA4KwRaAAoJ0d64AG3/vc/m5o0CSgzM19Op9VVAcDZI9AAMc7nk7p0ceubb+y6+OKg5s7NV6VK\nVlcFAOeGQAPEsGBQ6tcvXqtWOVS9elDvvpunCy/kjCYAkYdAA8Qo05Seey5O773nVIUKpubNy1eD\nBoQZAJGJQAPEINOUxo51ado0l1wuU7Nm5atpU8ZoAhC5CDRAjDkeZiZOjJPdburNNz268UauNQMg\nshFogBhSXJhp395vdVkAUGIOqwsAUD5MUxozxqVJk0Jh5q23PLrjDsIMgOjAHhogBhBmAEQ79tAA\nUc40pRdfdOn110NhZsoUj/78Z8IMgOhS4j00pmlq8ODBmjFjRmnUA6AUFRSErjPz+utxcjgIMwCi\nV4kCTVZWljp37qwPP/ywtOoBUEpyc6W0NLfmz3cqIcHUX/6ST5gBELVKdMhpzpw5Sk1NVe3atUur\nHgCl4OBBQw8+6NZXX9lVrVpQc+bk6+qruc4MgOh1xkCzcuVK9e7du8jzo0aN0rBhwyRJa9euPesF\nJiYmyOGwn0OJkaFGDQa/iQSxsJ62bZPuuEP64QcpOVlautSmhg0rWF3WWbHZDEmxsZ4iHesoMsTS\nejpjoGnZsqU2b95cags8fDiv1OYVLmrUqKTs7GNWl4EziIX1tG6dTV26uJWdbdMVVwQ0b16+EhNN\nZWdbXdnZCQZN2WxG1K+nSBcL21I0iMb1dLqAxmnbQJSYO9ehu+5KUHa2TTfc4NeiRQw0CSB2EGiA\nCOf3S0OHxmnAALd8PkOPPOLTvHn5qlzZ6soAoPxwHRoggh0+LHXv7taqVQ45nabGjPGqU6cCq8sC\ngHJXKoFmzJgxpTEbAOdg0yabHnnErW3bbKpePaiMDI9atGCQSQCxiUNOQIQxTWnGDKfatEnQtm02\nNW0a0LJleYQZADGNQ05ABPn5Z+mxx+L1j384JUlpaT69+KJXbrfFhQGAxQg0QIRYt86mnj3d2rXL\npkqVTE2axDAGAHAcgQYIcwUFUnq6S+PGuRQIGGrWLKApU/KVnMwp2QBwHIEGCGObN9vUv3+8/v3v\n0NW1e/f2acgQr1wuiwsDgDBDoAHCUEGB9PrrLo0f71JBgaF69YKaNMmjG2+k8RcAikOgAcLMb/fK\ndO7s0/DhXlWsaHFhABDGCDRAmMjJkcaPj9PUqU75/Ybq1g3qlVfYKwMAZ4NAA1jMNKUPPnBo2LA4\n7dljk2GY6tLFp2HD2CsDAGeLQANY6McfDQ0eHK9Vq0KbYrNmAY0d69FVVwUtrgwAIguBBrDAwYOG\nJk50KTPTqYICQ1Wrmho61KsHHyyQ3W51dQAQeQg0QDnKy5OmTXPptddcOnbMkGGYevBBn4YO9ala\nNa4rAwDni0ADlINAQHr3XYfGjg31yUhSq1Z+PfecV1dcweElACgpAg1Qhvx+6W9/c+iVV+KUlRUK\nMikpAQ0b5lXLlpy9BAClhUADlIGCAumvf3Vo0qQ4bdsWCjIXXRTUM894ddddftkY5x4AShWBBihF\n+fnSggVOvfaaSzt2hFJL/fpBPf64V3ff7ZfTaXGBABClCDRAKdi/39DMmU5lZjp18GAoyFxySSjI\npKb65WBLA4AyxdcsUALff2/TlClO/fWvTnm9hiSpadOA+vTx6Y47/JyCDQDlhEADnCOvV1qyxKHZ\ns536/PPQJmQYpm67rUC9exeoRYuADMPiIgEgxhBogLOUlWVo9myX3n3XoUOHQoeVEhJM3XdfgXr2\n9OmSS7iODABYhUADnMaRI9IHHzi1YIFDa9f+urk0aRJQWlqB7r67QJUqWVggAEASgQYooqBA+te/\n7FqwwKmPPnIU9sa43abuvNOvzp19atYsyGElAAgjBBpAoRCzerVdixc79I9/OHX4cCitGIapG27w\n6957C9SunZ/RrwEgTBFoELM8nlCIWbLEqQ8/dOjnn3/d5dKwYUD33efX3XcXqE4demMAINwRaBBT\n9u0ztHy5Q8uW2bVqlUN5eb+GmMsvD6hdO7/+/Ge/Lr+c8ZUAIJIQaBDVPB5p3Tq7Vq+2a/VqacOG\nk48ZNW0a0G23hUJMw4aEGACIVAQaRJVAQNq0yaZVqxxatcquL7+0y+P5dS+M223qxhsDuvVWv269\n1a9atTicBADRgECDiObzSd9+a9O6dXatXWvXp5+e3AsjSVdcEdANNwTUvr1LV1yRo4QEi4oFAJQZ\nAg0iyoEDhtavDwWYdevs+vrrk/fASKFRrW+80a8bbwzoT38KqEaN0F6YGjVcys62omoAQFkj0CBs\n/fyztGmTXf/+t02bNoXCy9attiLTXXZZQM2bB9S8eVB/+pNfyckcRgKAWEOggeVMU9q929D339v0\n73//GmB27CgaXtxuU1dffTzABHTNNQFdcIEFRQMAwgqBBuXGNKU9e0LB5b//PX6z67//tSknp+hl\nd91uU7/7XVBNmwaUkhL627hxUE6nBcUDAMIagQalyjSl7GxDW7fatG2bof/9z1Z427rVpmPHih8v\noFq1oBo1CiolJaiUlICaNg3q0kuDcvAvFABwFvi5wDnLyZF++smmn34y9NNPNm3ffnJwyc099SBH\nF1wQCi6/vR1v3AUA4HwQaHCSnBxp/35D+/bZtGuXod27i/49cuT0ozJWqWKqQYOg6tf/7c1UtWom\ngzoCAEodgSbKmaaUmysdOmTo0CFD2dmG9u+3af9+45fgYvxyP/TciUMBnEp8vKnatU3VqRNUnTqm\n6tULnhRgEhPL4YMBAHACAk2EOB5Mjh0zdPSooSNHpMOHDR0+bOjgQaPw/qFDv/49fr+g4Ox3icTH\nm0pKMnXhhaHAUru2qbp1T/7LXhYAQLgh0JShQEDKy5Py8gzl5Un5+b/+zc2Vjh41dOyY8UtI0Qn3\nDR07dvLrx45JweD5pYiEBFOJiaFbUtLxW7Dw/oUX/vq4YkURVgAAEScmAk0gEBqk0OeTfD5DXm/o\nvsdjnPRc6Hb8uV9fP/6c1xt67sSQkpdnyO+XjhxJOCmw5OWF5luaEhJMVaoUulWposKQcsEFodvx\n+8efr1Yt9Dc+vlTLAAAg7JR7oJk82Sm/PxQCCgokv1/FPi4oCAWR3z4Xul/0cUGBUTh96PlfQ0gg\nUB67HOxFnjEMUwkJoSDidksVKoT+Hn9cqZKpypVDt0qVfn0c+qvC8HL8dU5hBgCgeIZpmuV6vqxh\nJJ/ilUGSHv3lfidJq4uZpoWkeb/cnybppVPM6wdJLknfS7rthGX/eqtRY6oSE29RXJy0ZUsL+f3Z\nJ70uSZde+pCuvXaYXC5p7dqntWXLQtlsJ8/nwguT9dZbK+Xz5Wr9+iV69dWnZBgqnO64xYuXqnbt\nOvr558O65ZYbiq362WeH6e6775MkPfjgvfr++/8Umebmm/+fxo9/RZKUnv6KMjOnF5kmISFBq1d/\nKUlav/5L9ez5cLHLy8h4W1de2UySdO21V8nv9xeZpkeP3urZM7ReHnvsUa1evbLINCkpVyozc44k\nad68ORo3bnSxy1u5cq0qVqyobdv+p7vvbl/sNC+/PFG33PJ/kqR27f5Pe/bsLjLNXXfdo6FDR0iS\nRo4coffe+2uRaWrVqq0lS5ZJkv75z2V66qknZLMZCgZP/uf+t78tVnJyfeXk5KhlyxbF1jRo0DPq\n0OFBSVKXLg9q06Zvikxzww0t9corkyVJU6ZM1tSpbxaZxuFw6IsvvpYkffPNRj38cKdilzdlSoZ+\n//s//DLfPygvL6/INF26dFO/fo9JkgYOfEz/+teKItNcfnljzZmz4JfPOV+jRr1Q7PL++c/Vqlo1\nUbt3/6T27VsXO83IkWN1++1tJUl33dVWO3ZsLzJNu3Z36PnnQ9vk2LEvaf78uUWmqV69upYu/USS\ntHLlv/TEE/1Oen337p8kSatWfaHLLmson8+n6667utiaHntsoDp16iJJ6tGjizZsWF9kmmuvvU5v\nvDFNkjRjxlS98cZrxc5rw4ZvJUmbN3+nTp3uL3aa11+fouuu+5Mk6eab/6SjR48UmebBB9P0xBNP\nSZKefXaQli79sMg0l1xyqebPf1+StHjx+xoxYmixy/vww4+VlJSk/fv36/bbWxU7zYgRI9W+/Z2S\npPvuu1NZWT8WmaZ169s1atQ4SdLEiS9rzpzZRaapXLmK/vWvTyVJn3/+qfr27Vns8t5++1397ndX\nqEaNSrrooouLnaZPn/565JEev9zvri+++LzINNdc83tNnZr5yzwz9cor44ud1+effyWXy6UtW35Q\nhw6pxU4zcWK6Wra8+ZfPepMOHDhQZJr77uuop58eIkkaPnyIlixZVGSaiy66WO+993dJ0ocf/l1D\nhz5d7PIi6bv8xO+8aPku79Dh7mKXJ1mwhybUo/FrU+nxv9dfX6Drr/fI4ZDmzQvof/8zT3rdMEJj\n9gwcmCenU1q+3Kv584tOI0l//3uO3G6Xdu3KVdeuxTewTpzoUcuWoR+K1q2DOnCgaK679Va/nn7a\nK0kaPjyg/fuLzicx0dQNN0jZ2UH99JPJXhQAACxQ7ntosrOPlefiykWNGpWi8nNFG9ZT+Lvmmiay\n2QytW7fJ6lJwGmxLkSEa11ONGpVO+VrR0f8AAAAiDIEGAABEPAINAACIeAQaAAAQ8Qg0AAAg4hFo\nAABAxCPQAACAiEegAQAAEY9AAwAAIh6BBgAARLwSjTy0aNEizZgxQ4ZhyO12a8iQIUpJSSmt2gAA\nAM7KeQearVu3aty4cVq4cKGSkpK0cuVK9evXT5988kkplgcAAHBm533IyeVyaeTIkUpKSpIkNWnS\nRAcOHJDP5yu14gAAAM7GGffQrFy5Ur179y7y/KhRo3TnnXdKkkzT1OjRo9WqVSu5XK7Tzi8xMUEO\nh/08yw1fpxsBFOGD9RTebDZDEuspErCOIkMsrSfDNE2zJDPIy8vT4MGDtXfvXk2fPl2VK1c+7fTR\nNpS5FJ1DtEcj1lP4u+aaJrLZDK1bt8nqUnAabEuRIRrX0+kCWonOctq9e7c6dOggu92u2bNnnzHM\nAAAAlIXzbgr++eef9dBDDyk1NVV9+/YtzZoAAADOyXkHmrlz52rPnj1avny5li9fXvh8ZmamEhMT\nS6U4AACAs3HegaZ3797FNgsDAACUN64UDAAAIh6BBgAARDwCDQAAiHgEGgAAEPEINAAAIOIRaAAA\nQMQj0AAAgIhX4rGcAAAArMYeGgAAEPEINAAAIOIRaAAAQMQj0AAAgIhHoAEAABGPQAMAACIegaaU\nrVixQldffbXVZeAUFi1apD//+c+644471KFDB23atMnqkvCLTz75RO3bt1fr1q3Vv39/5eTkWF0S\nisE2FDli7feI69CUom3btql79+46cOCANm7caHU5+I2tW7cqLS1NCxcuVFJSklauXKnhw4frk08+\nsbq0mHfo0CG1bdtWc+fOVXJyssaNG6fc3FyNGDHC6tJwArahyBGLv0fsoSkl+fn5GjRokAYPHmx1\nKTgFl8ulkSNHKikpSZLUpEkTHThwQD6fz+LKsGbNGqWkpCg5OVmS1LFjRy1evFj8/1Z4YRuKDLH6\ne+SwuoBIsnLlSvXu3bvI86NGjdKnn36q+++/X40aNbKgMpzodOvpzjvvlCSZpqnRo0erVatWcrlc\n5V0ifmPv3r2qWbNm4eOaNWsqJydHubm5qlixooWV4UR169ZV3bp1JbENhbNhw4bF5O8RgeYctGzZ\nUps3by7y/Jw5c+RwOHTPPfdo165dFlSGE51qPR2Xl5enwYMHa+/evZo+fXo5VoZTCQaDxT5vs7ET\nORyxDYWvWP494tuiFLz33nvatGmT7rjjDvXo0UMej0d33HGH9u3bZ3Vp+I3du3erQ4cOstvtmj17\ntipXrmx1SZBUq1YtZWdnFz7et2+fqlSpooSEBAurQnHYhsJbLP8e0RRcynbt2qX27dvHTBNWJPn5\n55+Vmpqq1NRU9e3b1+pycIKDBw+qffv2euedd5ScnKwJEybowIEDGj16tNWl4QRsQ5El1n6POOSE\nmDF37lzt2bNHy5cv1/Llywufz8zMVGJiooWVoVq1aho9erT69++vgoICXXTRRRo7dqzVZeE32IYQ\nzthDAwAAIh49NAAAIOIRaAAAQMQj0AAAgIhHoAEAABGPQAMAACIegQYAAEQ8Ag0AAIh4BBoAABDx\n/j/EFjH8D2WcRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a0889b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.9232\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9566\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9658\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9698\n",
      "20 Batch accuracy: 0.98 Validation accuracy: 0.9706\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9692\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9702\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9698\n"
     ]
    }
   ],
   "source": [
    "means = mnist.train.images.mean(axis=0, keepdims=True)\n",
    "stds = mnist.train.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            X_val_scaled = (mnist.validation.images - means) / stds\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_val_scaled, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses tensorflow.contrib.layers.batch_norm() rather than tf.layers.batch_normalization() (which did not exist when this chapter was written). It is now preferable to use tf.layers.batch_normalization(), because anything in the contrib module may change or be deleted without notice. Instead of using the batch_norm() function as a regularizer parameter to the fully_connected() function, we now use batch_normalization() and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n",
    "\n",
    "・decay is renamed to momentum,\n",
    "\n",
    "・is_training is renamed to training,\n",
    "\n",
    "・updates_collections is removed: the update operations needed by batch normalization are added to the UPDATE_OPS collection and you need to explicity run these operations during training (see the execution phase below),\n",
    "\n",
    "・we don't need to specify scale=True, as that is the default.\n",
    "\n",
    "Also note that in order to run batch norm just before each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "Note: since the tf.layers.dense() function is incompatible with tf.contrib.layers.arg_scope() (which is used in the book), we now use python's functools.partial() function instead. It makes it easy to create a my_dense_layer() function that just calls tf.layers.dense() with the desired parameters automatically set (unless they are overridden when calling my_dense_layer()). As you can see, the code remains very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over again, we can use Python's partial() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are using tf.layers.batch_normalization() rather than tf.contrib.layers.batch_norm() (as in the book), we need to explicitly run the extra update operations needed by batch normalization (sess.run([training_op, extra_update_ops],...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8697\n",
      "1 Test accuracy: 0.8988\n",
      "2 Test accuracy: 0.9128\n",
      "3 Test accuracy: 0.9208\n",
      "4 Test accuracy: 0.9284\n",
      "5 Test accuracy: 0.9366\n",
      "6 Test accuracy: 0.9395\n",
      "7 Test accuracy: 0.9435\n",
      "8 Test accuracy: 0.9475\n",
      "9 Test accuracy: 0.9506\n",
      "10 Test accuracy: 0.9519\n",
      "11 Test accuracy: 0.9544\n",
      "12 Test accuracy: 0.9578\n",
      "13 Test accuracy: 0.9575\n",
      "14 Test accuracy: 0.9599\n",
      "15 Test accuracy: 0.9611\n",
      "16 Test accuracy: 0.9603\n",
      "17 Test accuracy: 0.9636\n",
      "18 Test accuracy: 0.9644\n",
      "19 Test accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a great accuracy for MNIST. Of course, if we train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.\n",
    "\n",
    "Note that we could also make the training operation depend on the update operations:\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        \n",
    "This way, we would just have to evaluate the training_op during training, TensorFlow would automatically run the update operations as well:\n",
    "\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "\n",
    "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If we want to reuse a pretrained neural network (see below), we must not forget these non-trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we apply gradient clipping. For this, we need to get the gradients, use the clip_by_value() function to clip them, then apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.3053\n",
      "1 Test accuracy: 0.796\n",
      "2 Test accuracy: 0.8807\n",
      "3 Test accuracy: 0.9024\n",
      "4 Test accuracy: 0.9121\n",
      "5 Test accuracy: 0.9194\n",
      "6 Test accuracy: 0.9246\n",
      "7 Test accuracy: 0.9302\n",
      "8 Test accuracy: 0.9331\n",
      "9 Test accuracy: 0.9403\n",
      "10 Test accuracy: 0.9432\n",
      "11 Test accuracy: 0.9457\n",
      "12 Test accuracy: 0.9476\n",
      "13 Test accuracy: 0.9482\n",
      "14 Test accuracy: 0.9519\n",
      "15 Test accuracy: 0.9542\n",
      "16 Test accuracy: 0.9554\n",
      "17 Test accuracy: 0.9533\n",
      "18 Test accuracy: 0.9585\n",
      "19 Test accuracy: 0.9608\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the graph's structure. The import_meta_graph() function does just that, loading the graph's operations into the default graph, and returning a Saver that we can then use to restore the model's state. Note that by default, a Saver saves the structure of the graph into a .meta file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get a handle on all the operations we will need for training. If we don't know the graph's structure, we can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/InTopK\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of operations! It's much easier to use TensorBoard to visualize the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know which operations we need, we can get a handle on them using the graph's get_operation_by_name() or get_tensor_by_name() methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are the author of the original model, we could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start a session, restore the model's state and continue training on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9593\n",
      "1 Test accuracy: 0.96\n",
      "2 Test accuracy: 0.9621\n",
      "3 Test accuracy: 0.9631\n",
      "4 Test accuracy: 0.9648\n",
      "5 Test accuracy: 0.965\n",
      "6 Test accuracy: 0.9663\n",
      "7 Test accuracy: 0.9648\n",
      "8 Test accuracy: 0.9655\n",
      "9 Test accuracy: 0.9655\n",
      "10 Test accuracy: 0.9676\n",
      "11 Test accuracy: 0.9671\n",
      "12 Test accuracy: 0.9674\n",
      "13 Test accuracy: 0.9676\n",
      "14 Test accuracy: 0.9681\n",
      "15 Test accuracy: 0.9685\n",
      "16 Test accuracy: 0.9685\n",
      "17 Test accuracy: 0.9689\n",
      "18 Test accuracy: 0.9697\n",
      "19 Test accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "documents",
   "language": "python",
   "name": "documents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
